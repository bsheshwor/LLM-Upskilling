{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Processing Using Spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Words\n",
    "   - Stop words are words that are filtered out before or after the natural language data(text) are processed.\n",
    "   - **stop words** typically refers to the most common words in a language.\n",
    "   - There is no universal list of **stop words** that is used by all NLP tools in common.\n",
    "   \n",
    "   \n",
    "   \n",
    "   - **what are stop words?**\n",
    "       - **Stopwords** are the **words** in any language which does not add much meaning to a sentence.\n",
    "       - They can safely be ignored without sacrificing the meaning of the sentence.\n",
    "       - For some search engines, these are some of the most common, short function words, such as the, is, at, which, and on.\n",
    "   \n",
    "   \n",
    "   - **when to remove stop words?**\n",
    "       - If we have a task of **text classification** or **sentiment analysis** then we should remove stop words as they do not provide any information to our model i.e. **keeping out unwanted words out of our corpus**.\n",
    "       - But, if we have the task of **language translation** then **stopwords** are useful, as they have to be translated along with other words.\n",
    "       - There is no hard and fast rule on when to remove stop words\n",
    "           1. Remove **stopwords** if task to be performed is one of **Language Classification**, **Spam Filtering**, **Caption Generation**, **Auto-Tag Generation**, **Sentiment analysis**, or something that is related to **text classification.**\n",
    "           2. Better not to remove **stopwords** if task to be performed is one of **Machine Translation**, **Question Answering problems**, **Text summarization**, **Language Modeling**.\n",
    "           \n",
    "           \n",
    "   - **Pros of Removing stop words**\n",
    "       -  **Stopwords** are often removed from the text before training deep learning and machine learning models since stop words occur in abundance, hence providing little to no unique information that can be used for classification or clustering.\n",
    "       - On removing **stopwords**, dataset size decreases, and the time to train the model also decreases without a huge impact on the accuracy of the model.\n",
    "       - **Stopword** removal can potentially help in improving performance, as there are fewer and only significant tokens left. Thus, the classification accuracy could be improved.\n",
    "       \n",
    "       \n",
    "       \n",
    "   - **Cons of Removing Stop Words**\n",
    "       - Improper selection and removal of **stop words** can change the meaning of our text. So we have to be careful in choosing our stop words.\n",
    "       - **Example: _This movie is not good_**\n",
    "           - If we **remove (not )** in pre-processing step the sentence (this movie is good) indicates that it is positive which is wrongly interpreted.\n",
    "           \n",
    "   \n",
    "   \n",
    "  \n",
    "   - **Removing Stop words using SpaCy Library**\n",
    "       - Comparing to **NLTK**, spacy got bigger set of stop words (326) than that of NLTK (179)\n",
    "       - **installation: (spacy, English Language Model)**\n",
    "           - _pip install -U spacy_\n",
    "           - _python -m spacy download en_core_web_sm_\n",
    "       - **Demo shown in below Cell:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in /home/anish/.local/lib/python3.8/site-packages (2.3.4)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/anish/.local/lib/python3.8/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/anish/.local/lib/python3.8/site-packages (from spacy) (3.0.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/lib/python3/dist-packages (from spacy) (1.17.4)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/anish/.local/lib/python3.8/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/anish/.local/lib/python3.8/site-packages (from spacy) (0.8.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy) (45.2.0)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /home/anish/.local/lib/python3.8/site-packages (from spacy) (7.4.3)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/anish/.local/lib/python3.8/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/anish/.local/lib/python3.8/site-packages (from spacy) (4.48.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy) (2.22.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/anish/.local/lib/python3.8/site-packages (from spacy) (0.7.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/anish/.local/lib/python3.8/site-packages (from spacy) (2.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/anish/.local/lib/python3.8/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/lib/python3/dist-packages (from spacy) (1.17.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/anish/.local/lib/python3.8/site-packages (from spacy) (2.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/anish/.local/lib/python3.8/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/anish/.local/lib/python3.8/site-packages (from spacy) (4.48.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/anish/.local/lib/python3.8/site-packages (from spacy) (3.0.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/lib/python3/dist-packages (from spacy) (1.17.4)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/anish/.local/lib/python3.8/site-packages (from spacy) (1.0.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/anish/.local/lib/python3.8/site-packages (from spacy) (0.8.0)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/anish/.local/lib/python3.8/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/anish/.local/lib/python3.8/site-packages (from spacy) (0.7.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/anish/.local/lib/python3.8/site-packages (from spacy) (2.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/anish/.local/lib/python3.8/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/anish/.local/lib/python3.8/site-packages (from spacy) (1.0.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.3; however, version 20.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "/usr/bin/python: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "#installation\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered sentence without stop words:\n",
      " ['Oh', 'man', ',', 'pretty', 'cool', '.', 'We', 'things', '.']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"\n",
    "    Removes stopwords passed from the text passed as an arguments\n",
    "    \n",
    "    Arguments:\n",
    "    text: raw text from where stopwords need to removed\n",
    "    \n",
    "    Returns:\n",
    "    tokens_without_sw: list of tokens of raw text without stopwords\n",
    "    \"\"\"\n",
    "    \n",
    "    # loading english language model of spacy\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    # getting list of default stop words in spaCy english model\n",
    "    stopwords =nlp.Defaults.stop_words\n",
    "    \n",
    "    # tokenize text\n",
    "    text_tokens = word_tokenize(text)\n",
    "    \n",
    "    # remove stop words:\n",
    "    tokens_without_sw = [word for word in text_tokens if word not in stopwords]\n",
    "    \n",
    "    # return list of tokens with no stop words\n",
    "    return tokens_without_sw\n",
    "\n",
    "# define sample text\n",
    "sample_text = \"Oh man, this is pretty cool. We will do more such things.\"\n",
    "\n",
    "# remove stopwords calling defined functions\n",
    "filtered_sentence = remove_stopwords(sample_text)\n",
    "print(\"filtered sentence without stop words:\\n\", filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "   - Tokenization refers to dividing the text into a sequence of words or sentences.\n",
    "   \n",
    "   \n",
    "   - **Word Tokenization**\n",
    "       - Word Tokenization simply means splitting sentence/text in words.\n",
    "       - Using attribute **token.text** to tokenize the **doc**\n",
    "     \n",
    "     \n",
    "   - **Sentence Tokenization**\n",
    "       - Sentence Tokenization is the process of splitting up strings into sentences.\n",
    "       - A sentence usually ends with a full stop (.), here focus is to study the structure of sentence in the analysis\n",
    "       - use **sents** attribute from spacy to identify the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Word tokens**\n",
      " ['Oh', 'man', ',', 'this', 'is', 'pretty', 'cool', '.', 'We', 'will', 'do', 'more', 'such', 'things', '.']\n"
     ]
    }
   ],
   "source": [
    "# Word Tokenization\n",
    "\n",
    "# import the spacy library\n",
    "import spacy\n",
    "\n",
    "# load the english model and initialize an object called 'nlp'\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize_word(text):\n",
    "    \"\"\"\n",
    "    Tokenize the text passed as an arguments into a list of words(tokens)\n",
    "    \n",
    "    Arguments:\n",
    "    text: raw text\n",
    "    \n",
    "    Returns:\n",
    "    words: list containing tokens in text\n",
    "    \"\"\"\n",
    "    # passing the text to nlp and initialize an object called 'doc'\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Tokenize the doc using token.text attribute\n",
    "    words = [token.text for token in doc]\n",
    "        \n",
    "    # return list of tokens\n",
    "    return words\n",
    "\n",
    "# define sample text\n",
    "sample_text =  \"Oh man, this is pretty cool. We will do more such things.\"\n",
    "\n",
    "# tokenize  words\n",
    "words = tokenize_word(sample_text)\n",
    "    \n",
    "# print tokens\n",
    "print(\"**Word tokens**\\n\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Sentence tokens**\n",
      " [Oh man, this is pretty cool., We will do more such things.]\n"
     ]
    }
   ],
   "source": [
    "# Sentence Tokenization\n",
    "\n",
    "# import the spacy library\n",
    "import spacy\n",
    "\n",
    "# load the english model and initialize an object called 'nlp'\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize_sentence(text):\n",
    "    \"\"\"\n",
    "    Tokenize the text passed as an arguments into a list of sentence\n",
    "    \n",
    "    Arguments:\n",
    "    text: raw text\n",
    "    \n",
    "    Returns:\n",
    "    sentences: list of sentences\n",
    "    \"\"\"\n",
    "    # passing the text to nlp and initialize an object called 'doc'\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # tokenize the sentence using sents attributes\n",
    "    sentences = list(doc.sents)\n",
    "    \n",
    "    # return tokenize sentence\n",
    "    return sentences\n",
    "\n",
    "# define sample text\n",
    "sample_text =  \"Oh man, this is pretty cool. We will do more such things.\"\n",
    "\n",
    "# tokenize sentence\n",
    "sentences = tokenize_sentence(sample_text)\n",
    "\n",
    "# print sentences\n",
    "print(\"**Sentence tokens**\\n\", sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Punctuation\n",
    "   - **punctuation** are special marks that are placed in a text to show the division between phrases and sentences.\n",
    "   - There are 14 punctuation marks that are commonly used in English grammar.\n",
    "   - They are, **period, question mark, exclamation point, comma, semicolon, colon, dash, hyphen, parentheses, brackets, braces, apostrophe, quotation marks, and ellipsis**.\n",
    "   - We can **remove punctuation** from text using  **is_punct** attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**list of tokens without punctutaions:**\n",
      " [Oh, man, this, is, pretty, cool, We, will, do, more, such, things]\n"
     ]
    }
   ],
   "source": [
    "# remove punctuations\n",
    "\n",
    "# import the spacy library\n",
    "import spacy\n",
    "\n",
    "# load the english model and initialize an object called 'nlp'\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    removes punctuation symbols present in the raw text passed as an arguments\n",
    "    \n",
    "    Arguments:\n",
    "    text: raw text\n",
    "    \n",
    "    Returns: \n",
    "    not_punctuation: list of tokens without punctuation\n",
    "    \"\"\"\n",
    "    # passing the text to nlp and initialize an object called 'doc'\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    not_punctuation = []\n",
    "    # remove the puctuation\n",
    "    for token in doc:\n",
    "        if token.is_punct == False:\n",
    "            not_punctuation.append(token)\n",
    "    \n",
    "    return not_punctuation\n",
    "\n",
    "# define sample text\n",
    "sample_text =  \"Oh man, this is pretty cool. We will do more such things.\"\n",
    "\n",
    "# remove punctuation\n",
    "not_punctuation = remove_punctuation(sample_text)\n",
    "\n",
    "print(\"**list of tokens without punctutaions:**\\n\", not_punctuation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see all the punctuation symbol is removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower Casing\n",
    "   - Converting word to lower case (NLP->nlp).\n",
    "   - **Q.Why Lower Casing**\n",
    "       - Words like **Book** and **book** mean the same,\n",
    "       - When not converted to the lower case those two are represented as two different words in the vector space model (resulting in more dimension).\n",
    "       - Higher the dimension, more computation resources are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "books are on the table.\n"
     ]
    }
   ],
   "source": [
    "def lower_casing(text):\n",
    "    \"\"\"\n",
    "    Accepts text as arguments and return text in lowercase\n",
    "    \n",
    "    Arguments:\n",
    "    text: raw text\n",
    "    \n",
    "    Returns:\n",
    "    text_to_lower: text converted to lower case\n",
    "    \"\"\"\n",
    "    text_to_lower = text.lower()\n",
    "    \n",
    "    return text_to_lower\n",
    "\n",
    "sample_text = \"Books are on the table.\"\n",
    "\n",
    "# lower casing\n",
    "print(lower_casing(sample_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "   - Lemmatization is the process of converting a word to its base form.\n",
    "   - For example, lemmatization would correctly identify the base form of **caring** to **care**\n",
    "   - Lemmatization can be carried out using the attribute **token.lemma_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The --> the\n",
      "Republican --> republican\n",
      "president --> president\n",
      "is --> be\n",
      "being --> be\n",
      "challenged --> challenge\n",
      "by --> by\n",
      "Democratic --> Democratic\n",
      "Party --> Party\n",
      "nominee --> nominee\n",
      "Joe --> Joe\n",
      "Biden --> Biden\n"
     ]
    }
   ],
   "source": [
    "# lemmatization\n",
    "\n",
    "# import the spacy library\n",
    "import spacy\n",
    "\n",
    "# load the english model and initialize an object called 'nlp'\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def lemmatization(text):\n",
    "    \"\"\"\n",
    "    obtain the lemma of the each token in the text, append to the list, and returns the list\n",
    "    \n",
    "    Arguments:\n",
    "    text: raw text\n",
    "    \n",
    "    Returns:\n",
    "    token_lemma_list: list containing token with its lemma\n",
    "    \"\"\"\n",
    "    \n",
    "    # passing the text to nlp and initialize an object called 'doc'\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    token_lemma_list = []\n",
    "    # Lemmatization\n",
    "    for token in doc:\n",
    "        token_lemma_list.append((token.text, token.lemma_))\n",
    "    \n",
    "    return token_lemma_list\n",
    "\n",
    "# define sample text\n",
    "sample_text = \"The Republican president is being challenged by Democratic Party nominee Joe Biden\"\n",
    "\n",
    "# Lemmatization\n",
    "token_lemma_list = lemmatization(sample_text)\n",
    "\n",
    "#printing\n",
    "for token_lemma in token_lemma_list:\n",
    "    print(token_lemma[0], '-->', token_lemma[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word **is** converted into **be**, **being** -> **be**, **challenged** -> **challenge.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS-Tagging\n",
    "   - Parts-of-speech tagging is the process of tagging words in textual input with their appropriate parts of speech.\n",
    "   - This is one of the core feature loaded into the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Antibiotics NOUN NNS\n",
      "do AUX VBP\n",
      "not PART RB\n",
      "help VERB VB\n",
      ", PUNCT ,\n",
      "as SCONJ IN\n",
      "they PRON PRP\n",
      "do AUX VBP\n",
      "not PART RB\n",
      "work VERB VB\n",
      "against ADP IN\n",
      "viruses NOUN NNS\n",
      ". PUNCT .\n"
     ]
    }
   ],
   "source": [
    "# import the spacy library\n",
    "import spacy\n",
    "\n",
    "# load the english model and initialize an object called 'nlp'\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def pos_tagging(text):\n",
    "    # passing the text to nlp and initialize an object called 'doc'\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    pos_list = []\n",
    "    for token in doc:\n",
    "        pos_list.append((token.text, token.pos_, token.tag_))\n",
    "    return pos_list\n",
    "\n",
    "# define sample text\n",
    "sample_text = 'Antibiotics do not help, as they do not work against viruses.'\n",
    "\n",
    "# pos_tagging\n",
    "pos_list = pos_tagging(sample_text)\n",
    "\n",
    "# display\n",
    "for pos in pos_list:\n",
    "    print(pos[0], pos[1], pos[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the words are tagged with appropriate parts of speech.  \n",
    "One important note, some words can be both noun or verb depending on context.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entity recognition\n",
    "   - It is the process of detecting the named entities such as the person name, the location name, the company name, the quantities and the monetary value.\n",
    "   - We can find the named entity using spaCy **ents** attribute class.\n",
    "   - **Entity attributes details**  \n",
    "   ![Entity attributes details](../images/entity_type_details.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Republican -> NORP\n",
      "Democratic Party -> ORG\n",
      "Joe Biden -> PERSON\n",
      "Barack -> GPE\n",
      "US -> GPE\n",
      "the 1970s -> DATE\n",
      "\n",
      "***VISUALIZING NAMED ENTITY RECOGNIZER***\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Republican\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " president is being challenged by \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Democratic Party\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " nominee \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Joe Biden\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", who                 is best known as \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Barack\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " Obama’s vice-president but has been in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    US\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " politics since \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the 1970s\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "# import the spacy library\n",
    "import spacy\n",
    "\n",
    "# import displacy\n",
    "from spacy import displacy\n",
    "\n",
    "\n",
    "# load the english model and initialize an object called 'nlp'\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def named_entity_recognition(text):\n",
    "    \"\"\"\n",
    "    returns entity_text and entity labels as a tuple\n",
    "    \n",
    "    Arguments:\n",
    "    text: raw text\n",
    "    \n",
    "    Returns:\n",
    "    entity_text_label: entity text and labels as a tuple\n",
    "    \"\"\"\n",
    "    # passing the text to nlp and initialize an object called 'doc'\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    #named entity recogniton using doc.ents\n",
    "    entity_text_label = []\n",
    "    \n",
    "    for entity in doc.ents:\n",
    "        entity_text_label.append((entity.text, entity.label_))\n",
    "        \n",
    "    return entity_text_label\n",
    "\n",
    "\n",
    "# define sample text\n",
    "sample_text = \"The Republican president is being challenged by Democratic Party nominee Joe Biden, who \\\n",
    "                is best known as Barack Obama’s vice-president but has been in US politics since the 1970s\"\n",
    "\n",
    "# Named Entity Recognition\n",
    "entity_text_label = named_entity_recognition(sample_text)\n",
    "\n",
    "# display entity text and label\n",
    "for text_label in entity_text_label:\n",
    "    print(text_label[0], '->', text_label[1])\n",
    "    \n",
    "# Visualizing the named entity description\n",
    "print(\"\\n***VISUALIZING NAMED ENTITY RECOGNIZER***\")\n",
    "displacy.render(nlp(sample_text), style = \"ent\",jupyter = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "224390953ffe4ffc953d2425899b26f28a6cfac249dfac765b546800b74315c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
