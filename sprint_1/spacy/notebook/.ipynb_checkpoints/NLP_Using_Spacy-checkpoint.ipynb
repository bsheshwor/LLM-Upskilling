{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction To Spacy  \n",
    "- At the center of spaCy is the object containing the processing pipeline. We usually call this variable **nlp.**\n",
    "- For example, \n",
    "    - to create an English nlp object, \n",
    "    - you can import the English language class **from spacy.lang.en** and instantiate it. You can use the **nlp object** like a function to analyze text.\n",
    "- spaCy supports variety of languages that are available in spacy.lang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World!\n"
     ]
    }
   ],
   "source": [
    "# Import the english language class\n",
    "from spacy.lang.en import English\n",
    "\n",
    "#create the nlp object\n",
    "nlp = English()\n",
    "\n",
    "#process a text for English language\n",
    "doc = nlp(\"Hello World!\")\n",
    "\n",
    "#print the document text\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liebe Grüße!\n"
     ]
    }
   ],
   "source": [
    "# Import the German Language Class\n",
    "from spacy.lang.de import German\n",
    "\n",
    "#create nlp object for German class\n",
    "nlp_german = German()\n",
    "\n",
    "# Process a text (this is German for: \"Kind regards!\")\n",
    "doc = nlp_german(\"Liebe Grüße!\")\n",
    "\n",
    "#print the document text\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Cómo estás?\n"
     ]
    }
   ],
   "source": [
    "    # Import the Spanish language class\n",
    "    from spacy.lang.es import Spanish\n",
    "\n",
    "    # Create the nlp object\n",
    "    nlp = Spanish()\n",
    "\n",
    "    # Process a text (this is Spanish for: \"How are you?\")\n",
    "    doc = nlp(\"¿Cómo estás?\")\n",
    "\n",
    "    # Print the document text\n",
    "    print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This nlp object,\n",
    "   - contains the processing pipeline\n",
    "   - includes language-specific rules for tokenization(tokeinzing text into words and punctuation)etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Doc Object**\n",
    "   - Doc object are created by processing a string of text with the nlp object.\n",
    "   - The obtain doc object behaves like a normal python sequence pipeline, and lets iterate over the tokens or getting tokens by its index.\n",
    "   - first token is at index 0, second token is at index 1 and so-on.\n",
    "   - When you call nlp on a string, spaCy first tokenizes the text and creates a document object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token number 1 is: Hello\n",
      "Token number 2 is: World\n",
      "Token number 3 is: !\n",
      "\n",
      "Accessing first token from doc object with the help of index: Hello\n"
     ]
    }
   ],
   "source": [
    "# doc object, created by processing a string of text with the nlp object\n",
    "doc = nlp(\"Hello World!\")\n",
    "\n",
    "#Iterate over tokens in a Doc\n",
    "token_count = 0\n",
    "for token in doc:\n",
    "    token_count += 1\n",
    "    print(f\"Token number {token_count} is: {token}\")\n",
    "    \n",
    "#access token with help of index\n",
    "print(f\"\\nAccessing first token from doc object with the help of index: {doc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Token Object**\n",
    "   - The token object represents the tokens in the document\n",
    "       - for example word or a punctuation character\n",
    "   - To get a token at specific position, you can index into the doc.\n",
    "   - **Token objects** also provide various attributes that let you access more information about the tokens.\n",
    "       - for example: the **.text** attribute returns the verbatim token text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World World\n"
     ]
    }
   ],
   "source": [
    "#Index into the Doc to get a single Token\n",
    "token = doc[1]\n",
    "\n",
    "#Get the token text via the .text attribute\n",
    "print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Span Object**\n",
    "- A span object is a slice of the document consisting of one or more tokens.\n",
    "- It's only a view of the Doc and doesn't contain any data itself.\n",
    "- To create a span, you can use python's slice notation.\n",
    "- For **example:**\n",
    "    - 1:3 will create a slice starting from the token at position 1, up to – but not including! – the token at position 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "World!\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Hello World!\")\n",
    "\n",
    "#A slice from the Doc is a Span object\n",
    "span = doc[1:3]\n",
    "\n",
    "#Get the span text via .text attribute\n",
    "print(span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lexical Attributes**\n",
    "- Here, we can see some available token attirbutes:\n",
    "    - **i** is the index of the token within the parent document.\n",
    "    - **text** return the token text\n",
    "    - **is_alpha** return boolean values indicating whether the token consists of alphabetic character\n",
    "        - **example:** the word **\"ten\"**\n",
    "    - **is_punct** return whether token is punctuation \n",
    "        - **example:** **\"one, zero\"**\n",
    "    - **like_num** \n",
    "        - **example:** a token **\"10\"**\n",
    "        \n",
    "- These attributes are also called **lexical attributes:** they refer to the entry in the vocabulary and don't depend on the token's context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc object, created by processing the string of text with the nlp object\n",
    "doc = nlp(\"It costs $5.\")\n",
    "\n",
    "#lexical attributes: i, text\n",
    "print(\"Index: \", [token.i for token in doc])\n",
    "print(\"Text: \", [token.text for token in doc])\n",
    "\n",
    "#lexical attributes: is_alpha, is_punct, like_num\n",
    "print(\"is_alpha: \", [token.is_alpha for token in doc])\n",
    "print(\"is_punct:\", [token.is_punct for token in doc])\n",
    "print(\"like_num:\", [token.like_num for token in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lexical attributes example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage found: 60\n",
      "Percentage found: 4\n"
     ]
    }
   ],
   "source": [
    "# Check whether the next token’s text attribute is a percent sign ”%“.\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "# Process the text\n",
    "doc = nlp(\n",
    "    \"In 1990, more than 60% of people in East Asia were in extreme poverty. \"\n",
    "    \"Now less than 4% are.\"\n",
    ")\n",
    "\n",
    "# Iterate over the tokens in the doc\n",
    "for token in doc:\n",
    "    # Check if the token resembles a number\n",
    "    if token.like_num:\n",
    "        # Get the next token in the document\n",
    "        next_token = doc[token.i+1]\n",
    "        # Check if the next token's text equals \"%\"\n",
    "        if next_token.text == \"%\":\n",
    "            print(\"Percentage found:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Models\n",
    "- Some of the most interesting things you can analyze are context-specific: \n",
    "    - for example, whether a word is a verb or whether a span of text is a person name.\n",
    "- Statistical models enable spaCy to predict linguistic attributes in context. This usually includes,\n",
    "    - Part-of-speech tags\n",
    "    - Syntatic dependencies\n",
    "    - Named entities.\n",
    "- Models are trained on large datasets of labeled example texts.\n",
    "- They can be updated with more examples to fine tune their predicions. for example, to perform better on your specific data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Packages**\n",
    "- spaCy provides a number of pre-trained model packages you can download using the spacy download command.\n",
    "-  For example, \n",
    "    - the **en_core_web_sm** package is a small English model that supports all core capabilities and is trained on web text.\n",
    "- **what's not included in the model packages?**\n",
    "   - _The labelled data that the model was trained on._ \n",
    "   - Once they’re trained, they use binary weights to make predictions. That’s why it’s not necessary to ship them with their training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en_core_web_sm==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.0 MB 1.2 MB/s eta 0:00:01    |██████████████▉                 | 5.6 MB 725 kB/s eta 0:00:09\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/anish/.local/lib/python3.8/site-packages (from en_core_web_sm==2.3.1) (2.3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.22.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.48.2)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/lib/python3/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.17.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.4)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.3)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (45.2.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/lib/python3/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.17.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/lib/python3/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.17.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.48.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.7.3)\n",
      "Building wheels for collected packages: en-core-web-sm\n",
      "  Building wheel for en-core-web-sm (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for en-core-web-sm: filename=en_core_web_sm-2.3.1-py3-none-any.whl size=12047105 sha256=ff7739dc8bf2d22b81285452364f9d3ec437a226763c4efb1d59b90d69cea844\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-8h6d3xha/wheels/ee/4d/f7/563214122be1540b5f9197b52cb3ddb9c4a8070808b22d5a84\n",
      "Successfully built en-core-web-sm\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-2.3.1\n",
      "\u001b[33mWARNING: You are using pip version 20.3; however, version 20.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The spacy.load method loads a model package by name and returns an nlp object.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The package provides the **binary weights** that enable spaCy to make predictions.\n",
    "- It also includes the **vocabulary**, and **meta information** to tell spaCy which language class to use and how to configure the processing pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**predicting Part-of-speech Tags**\n",
    "- Let's take a look at the model's predictions. In this example, we're using spaCy to predict part-of-speech tags, the word types in context.\n",
    "- First, we load the small English model and receive an nlp object.\n",
    "- Next, we're processing the text \"She ate the pizza\".\n",
    "- For each token in the doc, we can print the text and the .pos_ attribute, the predicted part-of-speech tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON\n",
      "ate VERB\n",
      "the DET\n",
      "pizza NOUN\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# load the small english model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# process text\n",
    "doc = nlp(\"She ate the pizza\")\n",
    "\n",
    "# Iterate over the tokens\n",
    "for token in doc:\n",
    "    #print the text and the predicted part-of-speech tag\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the model correctly predicted **ate** as a verb and **pizza** as a noun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicting Syntatic Dependencies**\n",
    "- In addition to part-of-speech tags, we can also predict how the words are related.\n",
    "- Example,\n",
    "    - whether a word is the subject of the sentence or an object.\n",
    "- The **.dep_** attribute returns the predicted dependency label.\n",
    "- The **.head** attribute returns the syntatic head token. You can also think of it as the parent token this word is attached to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "She PRON nsubj ate\n",
      "ate VERB ROOT ate\n",
      "the DET det pizza\n",
      "pizza NOUN dobj ate\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# load the small english model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# process text\n",
    "doc = nlp(\"She ate the pizza\")\n",
    "\n",
    "#predicting syntactic Dependencies\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note:_  \n",
    "   - _A **noun** is naming word. It is used to put a name to things to allow for ease of reference and a common frame of understanding._ \n",
    "   - _**Verbs** are actions or \"doing words\" used to express an action or state of being.\n",
    "   - _A **pronoun** (I, me, he, she, herself, you, it, that, they, each, few, many, who, whoever, whose, someone, everybody, etc.) is a word that takes the place of a noun_\n",
    "   - a **determiner** is a word that introduces a noun. It always comes before a noun, not after, and it also comes before any other adjectives used to describe. Determiners are required before a singular noun but are optional when it comes to introducing plural nouns.\n",
    "   - **adjective** is a word naming an attribute of a noun, such as sweet, red, or technical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependency label scheme** \n",
    "- To describe syntactic dependencies, spaCy uses a standardized label scheme. Here's an example of some common labels:\n",
    "    - The pronoun **She** is a nominal subject attached to the verb – in this case, to **ate**.\n",
    "    - The noun **pizza** is a direct object attached to the verb **ate**. It is eaten by the subject, **she**.\n",
    "    - The determiner **the**, also known as an article, is attached to the noun **pizza**.\n",
    "- **Dependecy label scheme**\n",
    "    - ![Dependency label scheme](images/dependency_label_scheme.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Predicting Named Entities**\n",
    "- Named entities are **real world objects** that are assigned a name- for example, a person, an organization or a country.\n",
    "- The **doc.ents** property lets you access the named entities predicted by the model.\n",
    "- **doc.ents** returns an iterator of Span objects, so we can print the **entity text** and the **entity label** using the **.label_** attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entity text: Apple   entity labels: ORG\n",
      "entity text: U.K.   entity labels: GPE\n",
      "entity text: $1 billion   entity labels: MONEY\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# load the small english model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# process a text\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "# Iterate over the predicted entities\n",
    "for ent in doc.ents:\n",
    "    #print the entity text and labels\n",
    "    print(\"entity text:\", ent.text, \"  entity labels:\", ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this case model is correctly predicting, **Apple** as an **Organization (_ORG_)**, **U.K.** as a **geopolitical entity (_GPE_)**, and **$1 billion** as **money**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**spaCy Explain method**\n",
    "- A quick tip: To get definitions for the most common tags and labels, you can use the **spacy.explain** helper function.\n",
    "- For example, \n",
    "    - \"GPE\" for geopolitical entity isn't exactly intuitive – but spacy.explain can tell you that it refers to countries, cities and states.\n",
    "    - The same work for part-of-speech tags and dependency labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entity explain, GPE means: Countries, cities, states\n",
      "POS explain, NNP means: noun, proper singular\n",
      "Dependency label explain, dobj means: direct object\n",
      "Dependency label explain, nsubj means: nominal subject\n"
     ]
    }
   ],
   "source": [
    "# get quick definitions of the most common tags and labels.\n",
    "\n",
    "# GPE, explain\n",
    "print(\"Named Entity explain, GPE means:\", spacy.explain(\"GPE\"))\n",
    "\n",
    "# part-of-speech tag, explain\n",
    "print(\"POS explain, NNP means:\", spacy.explain(\"NNP\"))\n",
    "\n",
    "# Dependency label, explain\n",
    "print(\"Dependency label explain, dobj means:\", spacy.explain(\"dobj\"))\n",
    "print(\"Dependency label explain, nsubj means:\", spacy.explain(\"nsubj\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rule-based matching\n",
    "- Compared to regular expressions, the matcher works with Doc and Token objects instead of only strings.\n",
    "- It's also more flexible: you can search for texts but also other lexical attributes.\n",
    "- You can even write rules that use the model's predictions.\n",
    "- Example:\n",
    "   - Find the word **duck** only if it's **verb**, not a **noun.** \n",
    "   - **duck(verb)** vs. **duck(noun)**\n",
    "   \n",
    "**match patterns**\n",
    "   - Match patterns are lists of dictionaries. Each dictionary describes one token. The keys are the names of token attributes, mapped to their expected values.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'TEXT': 'iPhone'}, {'TEXT': 'X'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# match exact token text\n",
    "[{\"TEXT\": \"iPhone\"}, {\"TEXT\": \"X\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example, we're looking for two tokens with the text **iphone** and **X**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'LOWER': 'iphone'}, {'LOWER': 'x'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# match lexical attributes\n",
    "[{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also match on other token attributes. Here, we're looking for two tokens whose lowercase forms equal **iphone** and **x**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'LEMMA': 'buy'}, {'POS': 'NOUN'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# match any token attributes\n",
    "[{\"LEMMA\": \"buy\"}, {\"POS\": \"NOUN\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can even write patterns using attributes predicted by the model. Here, we're matching a token with the lemma **buy**, plus a **noun**.\n",
    "- The Lemma is the base form, so this patten would match phrases like **buying milk** or **bought flowers**\n",
    "\n",
    "**Matcher Examples**\n",
    "- **Steps:**\n",
    "   1. _To use a pattern, we first import the matcher from spacy.matcher._\n",
    "   2. _Load a model and create the nlp object_ \n",
    "   3. _Initialize **matcher** with the shared vocabulary, **nlp.vocab**_\n",
    "   4. _Add a pattern using **matcher.add** method_\n",
    "       - _First argument: **unique ID** to  identify which pattern was matched._\n",
    "       - _Second argument: **optional callback** normally set to **None**_\n",
    "       - _Third argument: **pattern** desired pattern_\n",
    "   5. _Call **matcher** on any doc to match the pattern. This will return **the matches**_\n",
    "       - _When you call matcher on a doc, it returns a list of tuples_\n",
    "       - _Each tuple consists of three values:_ \n",
    "           - _**the match ID:** hash value of the pattern name_ , \n",
    "           - _**the start index:** start index of matched span_ , \n",
    "           - _**the end index:** end index of matched span_\n",
    "   6. _Iterate over the matches and create span object: a slice of the doc at the start and end index._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched text is: iphone X\n"
     ]
    }
   ],
   "source": [
    "# import spacy\n",
    "import spacy\n",
    "\n",
    "# Import the matcher from spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "# Load a model and create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to matcher\n",
    "pattern = [{\"TEXT\": \"iphone\"}, {\"TEXT\": \"X\"}]\n",
    "matcher.add(\"IPHONE_PATTERN\", None, pattern)\n",
    "\n",
    "# process some text\n",
    "doc = nlp(\"Upcoming iphone X release date leaked\")\n",
    "\n",
    "# call the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    matched_span = doc[start:end]\n",
    "    print(f\"matched text is: {matched_span}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**patterns-lexical attributes**\n",
    "- Now, lets see an example of a more complex pattern using lexical attributes\n",
    "- _Look for five tokens having following requirements:_\n",
    "   - _A token consisting of only digits_\n",
    "   - _Three case-insensitive tokens for **fifa**, **world**, **cup**_\n",
    "   - _Token that consists of punctuation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched patterns:  2018 FIFA World Cup:\n"
     ]
    }
   ],
   "source": [
    "# load the model and creat nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Initialize the matcher with the shared vocab\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "#Add pattern to the matcher\n",
    "pattern = [\n",
    "    {\"IS_DIGIT\": True},\n",
    "    {\"LOWER\": \"fifa\"},\n",
    "    {\"LOWER\": \"world\"},\n",
    "    {\"LOWER\": \"cup\"},\n",
    "    {\"IS_PUNCT\": True}\n",
    "]\n",
    "matcher.add(\"FIFA_PATTERN\", None, pattern)\n",
    "\n",
    "# process some text\n",
    "doc = nlp(\"2018 FIFA World Cup: France won!\")\n",
    "\n",
    "# call the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(\"matched patterns: \", matched_span)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched patterns: loved dogs\n",
      "matched patterns: love cats\n"
     ]
    }
   ],
   "source": [
    "# matching other token attributes\n",
    "\n",
    "\n",
    "# add pattern to matcher\n",
    "pattern = [\n",
    "    {\"LEMMA\": \"love\", \"POS\": \"VERB\"},\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]\n",
    "matcher.add(\"LOVE_PATTERNS\", None, pattern)\n",
    "\n",
    "# process some text\n",
    "doc = nlp(\"I loved dogs but now I love cats more.\")\n",
    "\n",
    "# call the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matches:\n",
    "    matched_span = doc[start:end]\n",
    "    print(\"matched patterns:\", matched_span)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In above example we're looking for two tokens:\n",
    "   - A verb with lemma **love**, followed by a noun\n",
    "- This pattern matched **loved dogs** and **love cats** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**patterns-using operators and quantifiers**\n",
    "- opertors and quantifiers let you define how often a token should be matched.\n",
    "   - They can be added using the **OP** key.\n",
    "- **lets define General function named _match_patterns()_ that matches the pattern as shown:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define General function to obtained matched patterns\n",
    "\n",
    "# import spacy\n",
    "import spacy\n",
    "\n",
    "# Import the matcher from spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "\n",
    "def match_patterns(pattern_id, pattern, some_text):\n",
    "    \"\"\"\n",
    "    matches pattern passed as argument to the text given\n",
    "    \n",
    "    Arguments:\n",
    "    pattern_id: Unique pattern identifier\n",
    "    pattern: desired pattern\n",
    "    some_text: text from where matche pattern to be extracted\n",
    "    \n",
    "    Returns:\n",
    "    matches: list of tuples of matched pattern\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load a model and create the nlp object\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    # Initialize the matcher with the shared vocab\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    \n",
    "    # add pattern to matcher\n",
    "    matcher.add(pattern_id, None, pattern)\n",
    "    \n",
    "    # process some text\n",
    "    doc = nlp(some_text)\n",
    "    \n",
    "    # call the matcher on the doc\n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    # Iterate over the matches\n",
    "    for match_id, start, end in matches:\n",
    "        matched_span = doc[start:end]\n",
    "        print(\"matched patterns:\", matched_span)\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched patterns: bought a smartphone\n",
      "matched patterns: buying apps\n"
     ]
    }
   ],
   "source": [
    "# patterns-using operators and quantifiers  \n",
    "\n",
    "# give patten id\n",
    "pattern_id = \"QUNATIFIERS\"\n",
    "\n",
    "# define pattern\n",
    "pattern = [\n",
    "    {\"LEMMA\": \"buy\"},\n",
    "    {\"POS\": \"DET\", \"OP\": \"?\"},  # optional: match 0 or 1 times\n",
    "    {\"POS\": \"NOUN\"}\n",
    "]\n",
    "# initialize some text\n",
    "some_text = \"I bought a smartphone. Now I'm buying apps.\"\n",
    "\n",
    "# call match_patterns\n",
    "matches = match_patterns(pattern_id, pattern, some_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In above example, the ? operator makes the determiner token optional, \n",
    "- so it will match a token with the lemma **buy**, an optional article and a noun.\n",
    "- **OP** can have four values:\n",
    "   - An **!** negates the token, so it's matched 0 times.\n",
    "   - A **+** matches a token 1 or more times.\n",
    "   - **?** matches 0 or 1 times\n",
    "   - And finally, an  \"*\" matches 0 or more times.\n",
    "   - ![operators_and_quantifiers](images/operators_and_quantifiers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.Write one pattern that only matches forms of “download” (tokens with the lemma “download”), followed by a token with the part-of-speech tag \"PROPN\" (proper noun).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched patterns: downloaded Fortnite\n",
      "matched patterns: downloading Minecraft\n"
     ]
    }
   ],
   "source": [
    "# write a pattern that matches a form \"download\" plus proper noun\n",
    "\n",
    "#set pattern id\n",
    "pattern_id = \"DOWNLOAD_THINGS_PATTERN\"\n",
    "\n",
    "#define pattern\n",
    "pattern = [\n",
    "    {\"LEMMA\": \"download\"},\n",
    "    {\"POS\": \"PROPN\"}\n",
    "]\n",
    "\n",
    "#some text\n",
    "some_text = \"i downloaded Fortnite on my laptop and can't open the game at all. Help? so when I was downloading Minecraft, I got the Windows version where it is the '.zip' folder and I used the default program to unpack it... do I also need to download Winzip?\"\n",
    "\n",
    "#call previously defined function\n",
    "matches = match_patterns(pattern_id, pattern, some_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.Write one pattern that matches adjectives (\"ADJ\") followed by one or two \"NOUN\"s (one noun and one optional noun).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matched patterns: beautiful design\n",
      "matched patterns: smart search\n",
      "matched patterns: optional voice\n",
      "matched patterns: optional voice responses\n"
     ]
    }
   ],
   "source": [
    "# pattern that matches a form, adjectives followed by one or two Noun\n",
    "\n",
    "# set pattern id\n",
    "pattern_id = \"ADJ_NOUN_PATTERN\"\n",
    "\n",
    "# define pattern\n",
    "pattern = [\n",
    "    {\"POS\": \"ADJ\"},\n",
    "    {\"POS\": \"NOUN\"},\n",
    "    {\"POS\": \"NOUN\", \"OP\":\"?\"}\n",
    "]\n",
    "\n",
    "# some text\n",
    "some_text = \"Features of the app include a beautiful design, smart search, automatic \\\n",
    "    labels and optional voice responses.\"\n",
    "\n",
    "# call previously defined function for pattern matching\n",
    "matches = match_patterns(pattern_id, pattern, some_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large-Scale data analysis with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structures: Vocab, Lexemes and StringStore \n",
    "**Vocab**\n",
    "- spaCy stores all shared data in a vocabulary, the **Vocab**.\n",
    "    - This includes words, but also the labels schemes for tags and entities.\n",
    "- To save memory, spaCy encodes all strings to hash values i.e. If a word occurs more than once, we don't need to save it every time.\n",
    "-  spaCy uses a hash function to generate an ID and stores the string only once in the string store.\n",
    "    - The string store is available as **nlp.vocab.strings.**\n",
    "- String Store is a lookup table that works in both directions.\n",
    "    - You can look up a string and get its hash, and look up a hash to get its string value. \n",
    "        - Internally, spaCy only communicates in hash IDs\n",
    "- However, Hash IDs can't be reversed, though. If a word is not in the vocabulary, there's no way to get its string. That's why we always need to pass around the shared vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'coffee'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load a model and create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Hashes can't be reversed – that's why we need to provide the shared vocab\n",
    "doc = nlp(\"i love coffee.\")\n",
    "\n",
    "# obtained hash from strings\n",
    "coffee_hash = nlp.vocab.strings[\"coffee\"]\n",
    "\n",
    "# obtained string from hash\n",
    "coffee_string = nlp.vocab.strings[coffee_hash]\n",
    "\n",
    "# show string\n",
    "coffee_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.Look up the string “cat” in nlp.vocab.strings to get the hash. Look up the hash to get back the string.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat hash is: 5439657043933447811\n",
      "string from cat_hash is: cat\n"
     ]
    }
   ],
   "source": [
    "# import model\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "doc = nlp(\"I have a cat\")\n",
    "\n",
    "# look up the hash for the word \"cat\"\n",
    "cat_hash = nlp.vocab.strings[\"cat\"]\n",
    "print(f\"cat hash is: {cat_hash}\")\n",
    "\n",
    "# look up the cat_hash to get the string\n",
    "cat_string = nlp.vocab.strings[cat_hash]\n",
    "print(f\"string from cat_hash is: {cat_string}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lexemes**\n",
    "- Lexemes are context-independent entries in the vocabulary.\n",
    "- You can get a lexeme by looking up a string or a hash ID in the vocab.\n",
    "- Lexemes expose attributes, just like tokens.\n",
    "- They hold context-independent information about a word, like the text, or whether the word consists of alphabetic characters.\n",
    "- Lexemes don't have part-of-speech tags, dependencies or entity labels. Those depend on the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text:  coffee \n",
      "hash:  3197928453018144401 \n",
      "is_alpha:  True\n"
     ]
    }
   ],
   "source": [
    "# create doc using nlp object\n",
    "doc = nlp(\"I love coffee\")\n",
    "\n",
    "# get a lexeme by looking up a string or a hash ID in the vocab\n",
    "lexeme = nlp.vocab[\"coffee\"]\n",
    "\n",
    "# print the lexical attributes\n",
    "print(\"text: \", lexeme.text, \"\\nhash: \", lexeme.orth, \"\\nis_alpha: \", lexeme.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- As seen above, lexemes contains the **context-independent** information about a word\n",
    "   - **lexeme.text->** word text \n",
    "   - **lexeme.orth->** the hash\n",
    "   - **is_alpha->** return true if text is alphabetical\n",
    "- As seen above, lexeme doesnot tell context-dependent part-of-speech tags, dependencies or entity labels.\n",
    "\n",
    "**vocab, hashes and lexemes**\n",
    "![vocab_hashes_lexemes](../images/vocab_hashes_lexemes.png)   \n",
    "- Explaination:\n",
    "   - **Doc** contains word in context– in this case, the tokens \"I\", \"love\" and \"coffee\" with their part-of-speech tags and dependencies. \n",
    "   - Each **token** refers to a **lexeme**, which knows the word's **hash ID**.\n",
    "   - To get the **string representation of the word**, spaCy looks up the hash in the **string store.**\n",
    "- Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_tags of She: PRON\n",
      "hash ID:  5252949303365547547\n",
      "String using Hash ID: She\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# load the small english model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# process text\n",
    "doc = nlp(\"She ate the pizza\")\n",
    "\n",
    "# access token 0 of doc to predict pos tags, doc contains word in context\n",
    "print(f\"pos_tags of She: {doc[0].pos_}\")\n",
    "\n",
    "# get lexeme by looking a string for token 0, lexeme doesnot contain word in context\n",
    "token_lexeme = nlp.vocab[doc[0].text]\n",
    "print(\"hash ID: \", token_lexeme.orth)\n",
    "\n",
    "# use hash ID using lexeme to get string representation\n",
    "print(\"String using Hash ID:\", nlp.vocab.strings[token_lexeme.orth])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structures: Doc, Span and Token\n",
    "- We take a look at most important data structures: the **Doc**, and its views **Token** and **Span**\n",
    "\n",
    "**The Doc Object**\n",
    "- The **Doc** is one of the central data structures in spaCy. It's created automatically when you process a text with the **nlp** object. But you can also instantiate the class manually.\n",
    "- After creating the **nlp** object, we can import the **Doc** class from **spacy.tokens**.\n",
    "- The **Doc** class takes three arguments:  \n",
    "   - **the shared vocab**\n",
    "   - **the words**\n",
    "   - **the spaces**\n",
    "- **Example,**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello world!\n"
     ]
    }
   ],
   "source": [
    "# create an nlp object\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Import the Doc Class\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "# The words and spaces to create the Doc from, desired text: \"Hello word!\"\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False] #spaces after each token, True if required False if doesnot require\n",
    "\n",
    "# create a doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces,)\n",
    "print(doc.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we've created a doc from three words.\n",
    "- The **spaces** are a list of boolean values indicating whether the word is followed by a space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The Span Object**\n",
    "- A **span** is a slice of **doc** consisting of one or more tokens.\n",
    "- The **span** takes at least three arguments:\n",
    "    - **doc** it refer to\n",
    "    - **start index**\n",
    "    - **end index**\n",
    "    - optional **label**\n",
    "- **Remember:** end index is exclusive  \n",
    "![The_span_object](../images/span_object.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "span without label:  Hello world\n",
      "Span with label:  Hello world\n"
     ]
    }
   ],
   "source": [
    "# Import the Doc and Span classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "# The words and spaces to create the doc from\n",
    "words = [\"Hello\", \"world\", \"!\"]\n",
    "spaces = [True, False, False]\n",
    "\n",
    "# create a Doc manually\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "\n",
    "# create a span manually\n",
    "span = Span(doc, 0, 2)\n",
    "print(\"span without label: \", span.text)\n",
    "\n",
    "# create a span with a label\n",
    "span_with_label = Span(doc, 0, 2, label=\"Greetings\")\n",
    "print(\"Span with label: \", span_with_label)\n",
    "\n",
    "# Add span to the doc.ents\n",
    "#The doc.ents are writable, so we can add entities manually by overwriting it with a list of spans.\n",
    "doc.ents = [span_with_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.Create the Doc and Span objects manually, and update the named entities-just like spaCy does behind the scenes.**\n",
    "   - _Import the Doc and Span classes from spacy.tokens_\n",
    "   - _Use the Doc class directly to create a doc from the words and spaces._\n",
    "   - _Create a Span for “David Bowie” from the doc and assign it the label \"PERSON\"_\n",
    "   - _Overwrite the doc.ents with a list of one entity, the “David Bowie” span._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc:  I like David Bowie\n",
      "Span text:  David Bowie \n",
      "Span label: PERSON\n",
      "[('David Bowie', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "#load model\n",
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "\n",
    "# Import the Doc and Span Classes\n",
    "from spacy.tokens import Doc, Span\n",
    "\n",
    "words = [\"I\", \"like\", \"David\", \"Bowie\"]\n",
    "spaces = [True, True, True, False]\n",
    "\n",
    "# create a doc from words and spaces\n",
    "doc = Doc(nlp.vocab, words=words, spaces=spaces)\n",
    "print(\"Doc: \", doc.text)\n",
    "\n",
    "# create a span for \"David Browie\" from the doc and assign it the label \"PERSON\"\n",
    "span = Span(doc, 2, 4, label=\"PERSON\")\n",
    "print(\"Span text: \", span.text, \"\\nSpan label:\", span.label_)\n",
    "\n",
    "# Add the span to the doc's entities\n",
    "#The doc.ents are writable, so we can add entities manually by overwriting it with a list of spans.\n",
    "doc.ents = [span]\n",
    "\n",
    "# Print entities' text and labels\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tips(Doc and Span)**\n",
    "- The Doc and Span are very powerful and optimized for performance. They give you access to all references and relationships of the words and sentences.\n",
    "- If your application needs to output strings, make sure to convert the doc as late as possible. If you do it too early, you'll lose all relationships between the tokens.\n",
    "- To keep things consistent, try to use built-in token attributes wherever possible. \n",
    "    - For example, token.i for the token index.\n",
    "- Also, don't forget to always pass in the shared vocab!    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.Analyze a text and collect all proper nouns that are followed by a verb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proper noun found: Berlin\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Berlin looks like a nice city\")\n",
    "\n",
    "for token in doc:\n",
    "    if token.pos_ == \"PROPN\":\n",
    "        if doc[token.i+1].pos_ == \"VERB\":\n",
    "            print(\"Proper noun found:\", token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vectors and semantic similarity\n",
    "- spaCy can compare two objects and predict how similar they are.\n",
    "    - for example, documents, spans or single tokens.\n",
    "- The **Doc**, **Token** and **Span** objects have a **.similarity** method that takes another object and returns a floating point number between 0 and 1, indicating how similar they are.\n",
    "- One thing that's very important: In order to use similarity, you need a larger spaCy model that has word vectors included.\n",
    "- For example, the medium or large English model – but not the small one. So if you want to use vectors, always go with a model that ends in \"md\" or \"lg\".\n",
    "- **Note:**\n",
    "    - Similarity is always subjective- whether \"dog\" and \"cat\" are similar really depends on how you're looking at it. \n",
    "    - spaCy’s similarity model usually assumes a pretty general-purpose definition of similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting en_core_web_md==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.3.1/en_core_web_md-2.3.1.tar.gz (50.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 50.8 MB 137 kB/s eta 0:00:01     |█████████████████████▏          | 33.6 MB 1.7 MB/s eta 0:00:11\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/anish/.local/lib/python3.8/site-packages (from en_core_web_md==2.3.1) (2.3.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (0.7.3)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (45.2.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.4)\n",
      "Requirement already satisfied: thinc<7.5.0,>=7.4.1 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (7.4.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/lib/python3/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.17.4)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (4.48.2)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (3.0.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.22.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.0.4)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/lib/python3/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.17.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.4)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.4)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/lib/python3/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.17.4)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.0.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (4.48.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (0.7.3)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (1.1.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (0.8.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (2.0.4)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/anish/.local/lib/python3.8/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_md==2.3.1) (3.0.4)\n",
      "\u001b[33mWARNING: You are using pip version 20.3; however, version 20.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "#Download medium english model with vectors\n",
    "!python3 -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Example:\n",
    "   - Let's say we want to find out whether two documents are similar.\n",
    "   - First, we load the medium English model, \"en_core_web_md\".\n",
    "   - We can then create two doc objects and use the first doc's **similarity** method to compare it to the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8627204117787385\n"
     ]
    }
   ],
   "source": [
    "#Similarity Examples no.1\n",
    "\n",
    "# Load a medium model with vectors\n",
    "import en_core_web_md\n",
    "nlp = en_core_web_md.load()\n",
    "\n",
    "# Compare two documents\n",
    "doc1 = nlp(\"I like fast food\")\n",
    "doc2 = nlp(\"I like pizza\")\n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7369546\n"
     ]
    }
   ],
   "source": [
    "# Compare two tokens\n",
    "doc = nlp(\"I like pizza and pasta\")\n",
    "token1 = doc[2]\n",
    "token2 = doc[4]\n",
    "print(token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the word vectors, the tokens \"pizza\" and \"pasta\" are kind of similar, and receive a score of 0.7.\n",
    "\n",
    "You can also use the similarity methods to compare different types of objects.  \n",
    "\n",
    "For example, a document and a token.\n",
    "\n",
    "    Here, the similarity score is pretty low and the two objects are considered fairly dissimilar.\n",
    "\n",
    "Here's another example comparing a span – \"pizza and pasta\" – to a document about McDonalds.\n",
    "\n",
    "    The score returned here is 0.61, so it's determined to be kind of similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32531983166759537\n"
     ]
    }
   ],
   "source": [
    "# Similarity examples no.2\n",
    "# Compare a document with a token\n",
    "doc = nlp(\"I like pizza\")\n",
    "token = nlp(\"soap\")[0]\n",
    "\n",
    "print(doc.similarity(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6199092090831612\n"
     ]
    }
   ],
   "source": [
    "# Compare a span with a document\n",
    "span = nlp(\"I like pizza and pasta\")[2:5]\n",
    "doc = nlp(\"McDonalds sells burgers\")\n",
    "\n",
    "print(span.similarity(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog dog 1.0\n",
      "dog cat 0.80168545\n",
      "dog banana 0.24327643\n",
      "cat dog 0.80168545\n",
      "cat cat 1.0\n",
      "cat banana 0.28154364\n",
      "banana dog 0.24327643\n",
      "banana cat 0.28154364\n",
      "banana banana 1.0\n"
     ]
    }
   ],
   "source": [
    "# Example\n",
    "import en_core_web_md\n",
    "\n",
    "nlp = en_core_web_md.load()  # make sure to use larger model!\n",
    "tokens = nlp(\"dog cat banana\")\n",
    "\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this case, the model’s predictions are pretty on point.  \n",
    "- A dog is very similar to a cat, whereas a banana is not very similar to either of them. \n",
    "- Identical tokens are obviously 100% similar to each other (just not always exactly 1.0, because of vector math and floating point imprecisions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q.How does spacy predict similarity?**\n",
    "- Similarity is determined using word vectors, multidimensional representations of meanings of words.\n",
    "- **word vectors** are generated using an algorithm like **Word2Vec** and a lots of text.\n",
    "- Vectors can be added to spaCy's statistical models.\n",
    "- Default: Similarity return by spaCy is the cosine similarity between two vectors- but this can be adjusted if necessary.\n",
    "- Vectors for objects consisting of several tokens, like the **Doc** and **Span**, default to the average of their token vectors.\n",
    "    - That's also why you usually get more value out of shorter phrases with fewer irrelevant words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**word vectors in spaCy**\n",
    "- To get an idea how word vectors look like, example:\n",
    "    - First, load the medium model again, which ships with word vectors.\n",
    "    - Next, process a text and look up a token's vector using the **.vector** attribute.\n",
    "    - The result is a 300-dimensional vector of the word \"banana\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.0228e-01 -7.6618e-02  3.7032e-01  3.2845e-02 -4.1957e-01  7.2069e-02\n",
      " -3.7476e-01  5.7460e-02 -1.2401e-02  5.2949e-01 -5.2380e-01 -1.9771e-01\n",
      " -3.4147e-01  5.3317e-01 -2.5331e-02  1.7380e-01  1.6772e-01  8.3984e-01\n",
      "  5.5107e-02  1.0547e-01  3.7872e-01  2.4275e-01  1.4745e-02  5.5951e-01\n",
      "  1.2521e-01 -6.7596e-01  3.5842e-01 -4.0028e-02  9.5949e-02 -5.0690e-01\n",
      " -8.5318e-02  1.7980e-01  3.3867e-01  1.3230e-01  3.1021e-01  2.1878e-01\n",
      "  1.6853e-01  1.9874e-01 -5.7385e-01 -1.0649e-01  2.6669e-01  1.2838e-01\n",
      " -1.2803e-01 -1.3284e-01  1.2657e-01  8.6723e-01  9.6721e-02  4.8306e-01\n",
      "  2.1271e-01 -5.4990e-02 -8.2425e-02  2.2408e-01  2.3975e-01 -6.2260e-02\n",
      "  6.2194e-01 -5.9900e-01  4.3201e-01  2.8143e-01  3.3842e-02 -4.8815e-01\n",
      " -2.1359e-01  2.7401e-01  2.4095e-01  4.5950e-01 -1.8605e-01 -1.0497e+00\n",
      " -9.7305e-02 -1.8908e-01 -7.0929e-01  4.0195e-01 -1.8768e-01  5.1687e-01\n",
      "  1.2520e-01  8.4150e-01  1.2097e-01  8.8239e-02 -2.9196e-02  1.2151e-03\n",
      "  5.6825e-02 -2.7421e-01  2.5564e-01  6.9793e-02 -2.2258e-01 -3.6006e-01\n",
      " -2.2402e-01 -5.3699e-02  1.2022e+00  5.4535e-01 -5.7998e-01  1.0905e-01\n",
      "  4.2167e-01  2.0662e-01  1.2936e-01 -4.1457e-02 -6.6777e-01  4.0467e-01\n",
      " -1.5218e-02 -2.7640e-01 -1.5611e-01 -7.9198e-02  4.0037e-02 -1.2944e-01\n",
      " -2.4090e-04 -2.6785e-01 -3.8115e-01 -9.7245e-01  3.1726e-01 -4.3951e-01\n",
      "  4.1934e-01  1.8353e-01 -1.5260e-01 -1.0808e-01 -1.0358e+00  7.6217e-02\n",
      "  1.6519e-01  2.6526e-04  1.6616e-01 -1.5281e-01  1.8123e-01  7.0274e-01\n",
      "  5.7956e-03  5.1664e-02 -5.9745e-02 -2.7551e-01 -3.9049e-01  6.1132e-02\n",
      "  5.5430e-01 -8.7997e-02 -4.1681e-01  3.2826e-01 -5.2549e-01 -4.4288e-01\n",
      "  8.2183e-03  2.4486e-01 -2.2982e-01 -3.4981e-01  2.6894e-01  3.9166e-01\n",
      " -4.1904e-01  1.6191e-01 -2.6263e+00  6.4134e-01  3.9743e-01 -1.2868e-01\n",
      " -3.1946e-01 -2.5633e-01 -1.2220e-01  3.2275e-01 -7.9933e-02 -1.5348e-01\n",
      "  3.1505e-01  3.0591e-01  2.6012e-01  1.8553e-01 -2.4043e-01  4.2886e-02\n",
      "  4.0622e-01 -2.4256e-01  6.3870e-01  6.9983e-01 -1.4043e-01  2.5209e-01\n",
      "  4.8984e-01 -6.1067e-02 -3.6766e-01 -5.5089e-01 -3.8265e-01 -2.0843e-01\n",
      "  2.2832e-01  5.1218e-01  2.7868e-01  4.7652e-01  4.7951e-02 -3.4008e-01\n",
      " -3.2873e-01 -4.1967e-01 -7.5499e-02 -3.8954e-01 -2.9622e-02 -3.4070e-01\n",
      "  2.2170e-01 -6.2856e-02 -5.1903e-01 -3.7774e-01 -4.3477e-03 -5.8301e-01\n",
      " -8.7546e-02 -2.3929e-01 -2.4711e-01 -2.5887e-01 -2.9894e-01  1.3715e-01\n",
      "  2.9892e-02  3.6544e-02 -4.9665e-01 -1.8160e-01  5.2939e-01  2.1992e-01\n",
      " -4.4514e-01  3.7798e-01 -5.7062e-01 -4.6946e-02  8.1806e-02  1.9279e-02\n",
      "  3.3246e-01 -1.4620e-01  1.7156e-01  3.9981e-01  3.6217e-01  1.2816e-01\n",
      "  3.1644e-01  3.7569e-01 -7.4690e-02 -4.8480e-02 -3.1401e-01 -1.9286e-01\n",
      " -3.1294e-01 -1.7553e-02 -1.7514e-01 -2.7587e-02 -1.0000e+00  1.8387e-01\n",
      "  8.1434e-01 -1.8913e-01  5.0999e-01 -9.1960e-03 -1.9295e-03  2.8189e-01\n",
      "  2.7247e-02  4.3409e-01 -5.4967e-01 -9.7426e-02 -2.4540e-01 -1.7203e-01\n",
      " -8.8650e-02 -3.0298e-01 -1.3591e-01 -2.7765e-01  3.1286e-03  2.0556e-01\n",
      " -1.5772e-01 -5.2308e-01 -6.4701e-01 -3.7014e-01  6.9393e-02  1.1401e-01\n",
      "  2.7594e-01 -1.3875e-01 -2.7268e-01  6.6891e-01 -5.6454e-02  2.4017e-01\n",
      " -2.6730e-01  2.9860e-01  1.0083e-01  5.5592e-01  3.2849e-01  7.6858e-02\n",
      "  1.5528e-01  2.5636e-01 -1.0772e-01 -1.2359e-01  1.1827e-01 -9.9029e-02\n",
      " -3.4328e-01  1.1502e-01 -3.7808e-01 -3.9012e-02 -3.4593e-01 -1.9404e-01\n",
      " -3.3580e-01 -6.2334e-02  2.8919e-01  2.8032e-01 -5.3741e-01  6.2794e-01\n",
      "  5.6955e-02  6.2147e-01 -2.5282e-01  4.1670e-01 -1.0108e-02 -2.5434e-01\n",
      "  4.0003e-01  4.2432e-01  2.2672e-01  1.7553e-01  2.3049e-01  2.8323e-01\n",
      "  1.3882e-01  3.1218e-03  1.7057e-01  3.6685e-01  2.5247e-03 -6.4009e-01\n",
      " -2.9765e-01  7.8943e-01  3.3168e-01 -1.1966e+00 -4.7156e-02  5.3175e-01]\n"
     ]
    }
   ],
   "source": [
    "# Load a larger model with vectors\n",
    "nlp = en_core_web_md.load()\n",
    "\n",
    "doc = nlp(\"I have a banana\")\n",
    "# Access the vector via the token.vector attribute\n",
    "print(doc[3].vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Similarity depends on the application context**\n",
    "- Predicting similarity can be useful for many types of applications:\n",
    "    - To recommend user similar texts based on the ones they have read.\n",
    "    - It can also be helpful to **flag duplicate content**, like posts on an online platform\n",
    "- There is no objective definition of word **Similarity**, it always depends on the context and what your applications needs to do.\n",
    "- **Example:**\n",
    "    - spaCy's default word vectors assign a very high similarity score to **I like cats** and **I hate cats**.\n",
    "    - This makes sense, because both texts express sentiment about cats.\n",
    "    - But in a different application context, you might want to consider the phrases as very **dissimilar**, because they talk about opposite sentiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combining models and rules**\n",
    "- **statisticals model**\n",
    "    - Statistical models are useful if your application needs to be able to generalize based on a few examples.\n",
    "        - For instance, detecting product or person names usually benefits from a statistical model.\n",
    "    - To do this, you would use spaCy's entity recognizer, dependency parser, or part-of-speech tagger.\n",
    "- **Rule-based approaches**\n",
    "    - Rule-based approaches on the other hand come in handy if there's a more or less finite number of instances you want to find. \n",
    "        - For example, all countries or cities of the world, drug names or even dog breeds.\n",
    "    - In spaCy, you can achieve this with custom tokenization rules, as well as the matcher and phrase matcher.  \n",
    "    - ![statistical_predictions_vs_rules](../images/statistical_predictions_vs_rules.png)\n",
    "\n",
    "- **Debugging Patterns**  \n",
    "    ![Debugging_Patterns](../images/debugging_patterns_1.png)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Efficient phrase matching**\n",
    "   - Sometimes it’s more efficient to match exact strings instead of writing patterns describing the individual tokens.\n",
    "   - The phrase matcher is another helpful tool to find sequences of words in your data.\n",
    "   - It performs keyword search on the document, but instead of only finding strings, it gives you direct access to the tokens in context.\n",
    "   - It takes **Doc** objects as patterns.\n",
    "   - It's also really fast.\n",
    "       - This makes it very useful for matching large dictionaries and word lists on large volumes of text.\n",
    "   - **Examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched span:  Golden Retriever\n"
     ]
    }
   ],
   "source": [
    "# import PhraseMatcher\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "#Instead of a list of dictionaries, we pass in a Doc object as the pattern.\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "pattern = nlp(\"Golden Retriever\")\n",
    "matcher.add(\"DOG\", None, pattern)\n",
    "doc = nlp(\"I have a Golden Retriever\")\n",
    "\n",
    "# Iterate over the matches\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    # Get the matched span\n",
    "    span = doc[start:end]\n",
    "    print(\"Matched span: \", span.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Efficient Phrase matching** is especially true for finite categories of things- like all countries of the world.\n",
    "- **Example:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Span's root head token, and span text**\n",
      "help --> Czech Republic\n",
      "\n",
      "\n",
      "**Span's root head token, and span text**\n",
      "protect --> Slovakia\n",
      "\n",
      "\n",
      "**Printing and labels:**\n",
      "[('Czech Republic', 'GPE'), ('Slovakia', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "patterns = [nlp(\"Czech Republic\"), nlp(\"Slovakia\")]\n",
    "matcher.add(\"COUNTRY\", None, *patterns) #*patterns-> Czech Republic Slovakia\n",
    "\n",
    "# create a doc and reset entities\n",
    "doc = nlp(\"Czech Republic may help Slovakia protect its airspace\")\n",
    "doc.ents = []\n",
    "\n",
    "# Iterate over the matches\n",
    "for match_id, start, end in matcher(doc):\n",
    "    # Create a Span with the label for \"GPE\"\n",
    "    span = Span(doc, start, end, label=\"GPE\")\n",
    "   \n",
    "    # overwrite the doc.ents and add the span\n",
    "    doc.ents = list(doc.ents)+[span]\n",
    "    \n",
    "    # Get the span's root head token \n",
    "    span_root_head = span.root.head\n",
    "    \n",
    "    # print the text of the span root's head token and the span text\n",
    "    print(\"**Span's root head token, and span text**\")\n",
    "    print(span_root_head.text, \"-->\", span.text)\n",
    "    print(\"\\n\")\n",
    "# Print the entities and labels in the documents\n",
    "print(\"**Printing and labels:**\")\n",
    "print([(ent.text, ent.label_)for ent in doc.ents if ent.label_==\"GPE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Pipelines\n",
    "- About spaCy's processing pipeline.\n",
    "   - **processing pipelines:** a series of function applied to a doc attributes like **part-of-speech tags**, **dependency labels**, or **named entities** \n",
    "- Learn what goes under the hood when you process a text.\n",
    "- How to write your own components and add them to the pipeline, and \n",
    "- How to use custom attributes to add your own metadata to the documents, spans and tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What happens when you call nlp?**\n",
    "   - First the tokenizer is applied to turn the string of text into a **Doc** object (i.e Tokenize text).\n",
    "   - Next, a series of pipeline components is applied to the **doc** in order. They include:\n",
    "       - **tagger**\n",
    "       - **The parser**\n",
    "       - **The entity recognizer** and so on....\n",
    "   - Finally, the processed doc is returned, so you can work with it.\n",
    "   - **Image showing pipelines**  \n",
    "   ![nlp_pipelines](../images/call_nlp_desc.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Built-in pipeline components**\n",
    "- spaCy ships with the following built-in pipeline components.\n",
    "    - The **part-of-speech tagger** sets the **token.tag** and **token.pos** attributes.\n",
    "    - The **dependency parser** adds the **token.dep** and **token.head** attributes and is also responsible for **detecting sentences** and **base noun phrases**, also known as **noun chunk.**\n",
    "    - The **named entity recognizer** adds the detected entities to the **doc.ents** property.\n",
    "        - It also sets the **entity type** attributes on the token that indicate if a token is part of an entity or not.\n",
    "    - Finally, **text classifier** sets category labels that apply to the whole text, and adds them to the **doc.cats** property\n",
    "    - Because text categories are always very specific, the text classifier is not included in any of the pretrained models by default. But you can use it to train your own system.\n",
    "    - **Image View: Built-in Pipeline Components**  \n",
    "        ![builtin_pipeline_components](../images/builtin_pipeline_components.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pipeline attributes**\n",
    "- To see the names of the pipeline components present in the current nlp object, you can use the **nlp.pipe_names** attribute.\n",
    "- For a list of **component name** and **component function** tuples, you can use the **nlp.pipeline** attribute.\n",
    "    - The **component funtions** are the functions applied to the doc to process it and set attributes- example: part-of-speech tags or named entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**The list of pipeline component names is:**\n",
      " ['tagger', 'parser', 'ner']\n",
      "\n",
      "**The list of name and component tuples is:**\n",
      " [('tagger', <spacy.pipeline.pipes.Tagger object at 0x7f0086788970>), ('parser', <spacy.pipeline.pipes.DependencyParser object at 0x7f00837d10a0>), ('ner', <spacy.pipeline.pipes.EntityRecognizer object at 0x7f00837d11c0>)]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# list of pipeline component names\n",
    "print(f\"**The list of pipeline component names is:**\\n {nlp.pipe_names}\")\n",
    "\n",
    "# list of (name, component) tuples\n",
    "print(f\"\\n**The list of name and component tuples is:**\\n {nlp.pipeline}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Amazing** ,   \n",
    "Whenever you're unsure about the current pipeline, you can inspect it by printing **nlp.pipe_names** or **nlp.pipeline** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Pipeline Components\n",
    "- Custome pipeline components is the powerful feature of spaCy's\n",
    "- Custome pipeline components let you add your own function to the spaCy pipeline that is executed when you call the **nlp** object on a text.\n",
    "\n",
    "**Q. Why Custom Components?**\n",
    "- Custom components are executed automatically when you call the **nlp** object on a text.\n",
    "- They're useful for adding your own custom metadata to documents and tokens.\n",
    "- You can also use **custom components** to update built-in attributes, like the named entity spans i.e **doc.ents**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Anatomy (Structure) of a custom component**\n",
    "- a **custom pipeline component** is a function or callable that takes a doc, modifies it and return it, so it can be processed by the next component in the pipeline.\n",
    "- Components can be added to the pipeline using **nlp.add_pipe** method.\n",
    "   - This **method** takes at least one argument: **the component function**. \n",
    "- Additionally, to specify **where** to add the component in the pipeline, you can use the following **keyword** shown:  \n",
    "    ![custom_component_loc](../images/custom_component_loc.png)  \n",
    "    - Setting last to True will add the component last in the pipeline. This is the default behavior.\n",
    "    - Setting first to True will add the component first in the pipeline, right after the tokenizer.\n",
    "    - The before and after arguments let you define the name of an existing component to add the new component before or after. For example, before=\"ner\" will add it before the named entity recognizer.\n",
    "- **Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline: ['custom_component', 'tagger', 'parser', 'ner']\n",
      "**Doc length from custom pipeline**: 6\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# create the nlp object\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define Custom Components\n",
    "def custom_component(doc):\n",
    "    \"\"\"\n",
    "    print the doc's length\n",
    "    \n",
    "    Arguments:\n",
    "    doc: document object\n",
    "    \n",
    "    Returns:\n",
    "    doc: Document object after adding cutom components\n",
    "    \"\"\"\n",
    "    # print the doc's length\n",
    "    print(\"**Doc length from custom pipeline**:\", len(doc))\n",
    "    \n",
    "    # return the doc object\n",
    "    return doc\n",
    "\n",
    "# Add the custom component in first of the pipeline\n",
    "nlp.add_pipe(custom_component, first=True)\n",
    "\n",
    "# print the pipeline component names\n",
    "print(\"Pipeline:\", nlp.pipe_names)\n",
    "\n",
    "# process text\n",
    "doc = nlp(\"My name is anish thapaliya.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, when we process the text using **nlp** object, the **custom component** is applied to the doc and the length of the document is printed  \n",
    "\n",
    "**Q. Write a custom component that uses the PhraseMatcher to find animal names in the document and adds the matched spans to doc.ents.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "animal patterns: [Golden Retriever, cat, turtle, Rattus norvegicus]\n",
      "added custom components: ['tagger', 'parser', 'ner', 'animal_component']\n",
      "[('cat', 'ANIMAL'), ('Golden Retriever', 'ANIMAL')]\n",
      "(cat, Golden Retriever)\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "animals = [\"Golden Retriever\", \"cat\", \"turtle\", \"Rattus norvegicus\"]\n",
    "\n",
    "animal_patterns = list(nlp.pipe(animals))\n",
    "\n",
    "print(\"animal patterns:\", animal_patterns)\n",
    "\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "\n",
    "matcher.add(\"ANIMAL\", None, *animal_patterns)\n",
    "\n",
    "# Define the custom components\n",
    "def animal_component(doc):\n",
    "    matches = matcher(doc)\n",
    "    \n",
    "    # create a span for each match and assign a label \"ANIMAL\"\n",
    "    span = [Span(doc, start, end, label=\"ANIMAL\") for match_id, start, end in matches]\n",
    "    \n",
    "    # overwrite the doc.ents with the matched span\n",
    "    doc.ents = span\n",
    "    \n",
    "    return doc\n",
    "\n",
    "# Add the component to the pipeline after ner\n",
    "nlp.add_pipe(animal_component, after='ner')\n",
    "print(\"added custom components:\", nlp.pipe_names)\n",
    "\n",
    "# process the text and print the text and label for the doc.ents\n",
    "doc = nlp(\"I have a cat and a Golden Retriever\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])\n",
    "print(doc.ents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Custom Attributes \n",
    "- Adding custom attributes to the **DOC**, **Token**, and **Span** objects to store custom data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a neural network model\n",
    "   - Focused specially on named entity recognizer.\n",
    "   - Learn how to update spaCy's statistical models to customize them for your use case.\n",
    "      - example: to predict new entity type in online comments.\n",
    "   - Write your own training loop from scratch, and understand the basics of how training works. \n",
    "\n",
    "**Why update the statistical model?**\n",
    "   - Statistical models make predictions based on the examples they were trained on.\n",
    "   - You can usually make the model more accurate by showing it examples from your domain.\n",
    "   - You often also want to predict categories specific to your problem, so the model needs to learn about them.\n",
    "   - This is essential for text classification, very useful for entity recognition and a little less critical for tagging and parsing.\n",
    "\n",
    "**How training works**\n",
    "   - **Steps**\n",
    "       1. **Initialize** the model weights randomly with nlp.begin_training\n",
    "       2. **Predict** a few examples with the current weights by calling **nlp.update**\n",
    "       3. **Compare** prediction with true labels\n",
    "       4. **Calculate** how to change weights to improve predictions\n",
    "       5. **Update** weights slightly\n",
    "       6. Go back to 2.\n",
    "   - **Explanation**  \n",
    "       ![training_mechanism](../images/training_process.png)\n",
    "       - The training data are the examples we want to update the model with.\n",
    "       - The text should be a sentence, paragraph or longer document. For the best results, it should be similar to what the model will see at runtime.\n",
    "       - The label is what we want the model to predict. This can be a text category, or an entity span and its type.\n",
    "       - The gradient is how we should change the model to reduce the current error. It's computed when we compare the predicted label to the true label.\n",
    "       - After training, we can then save out an updated model and use it in our application.\n",
    "       \n",
    "**Creating Training Data(1)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iPhone X]\n",
      "[iPhone X]\n",
      "[iPhone X]\n",
      "[iPhone 8]\n",
      "[iPhone 11, iPhone 8]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en import English\n",
    "\n",
    "TEXTS = ['How to preorder the iPhone X', 'iPhone X is coming', 'Should I pay $1,000 for the iPhone X?', \n",
    "         'The iPhone 8 reviews are here', \"iPhone 11 vs iPhone 8: What's the difference?\", \n",
    "         'I need a new phone! Any tips?']\n",
    "\n",
    "nlp = English()\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Two tokens whose lowercase forms match \"iphone\" and \"x\"\n",
    "pattern1 = [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "\n",
    "# Token whose lowercase form matches \"iphone\" and a digit\n",
    "pattern2 = [{\"LOWER\": \"iphone\"}, {\"IS_DIGIT\": True}]\n",
    "\n",
    "# Add patterns to the matcher and check the result\n",
    "matcher.add(\"GADGET\", None, pattern1, pattern2)\n",
    "\n",
    "# create a doc object for each text using nlp.pipe(gives doc object for each text in list)\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    print([doc[start:end] for match_id, start, end in matcher(doc)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Creating Training Data(2)**\n",
    "   - Create a doc object for each text using nlp.pipe.\n",
    "   - Match on the doc and create a list of matched spans.\n",
    "   - Get (start character, end character, label) tuples of matched spans.\n",
    "   - Format each example as a tuple of the text and a dict, mapping \"entities\" to the entity tuples.\n",
    "   - Append the example to TRAINING_DATA and inspect the printed data.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('How to preorder the iPhone X', {'entities': [(20, 28, 'GADGET')]})\n",
      "('iPhone X is coming', {'entities': [(0, 8, 'GADGET')]})\n",
      "('Should I pay $1,000 for the iPhone X?', {'entities': [(28, 36, 'GADGET')]})\n",
      "('The iPhone 8 reviews are here', {'entities': [(4, 12, 'GADGET')]})\n",
      "(\"iPhone 11 vs iPhone 8: What's the difference?\", {'entities': [(0, 9, 'GADGET'), (13, 21, 'GADGET')]})\n",
      "('I need a new phone! Any tips?', {'entities': []})\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.lang.en import English\n",
    "\n",
    "# list of texts\n",
    "TEXTS = ['How to preorder the iPhone X', 'iPhone X is coming', 'Should I pay $1,000 for the iPhone X?', \n",
    "         'The iPhone 8 reviews are here', \"iPhone 11 vs iPhone 8: What's the difference?\", \n",
    "         'I need a new phone! Any tips?']\n",
    "\n",
    "nlp = English()\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern1 = [{\"LOWER\": \"iphone\"}, {\"LOWER\": \"x\"}]\n",
    "pattern2 = [{\"LOWER\": \"iphone\"}, {\"IS_DIGIT\": True}]\n",
    "matcher.add(\"GADGET\", None, pattern1, pattern2)\n",
    "\n",
    "TRAINING_DATA = []\n",
    "\n",
    "# create doc object for each text in TEXTS\n",
    "for doc in nlp.pipe(TEXTS):\n",
    "    # match on the doc and create a list of matched spans\n",
    "    spans = [doc[start:end] for match_id, start, end in matcher(doc)]\n",
    "    \n",
    "    # Get (start character, end character, label) tuples of matches\n",
    "    entities = [(span.start_char, span.end_char, \"GADGET\") for span in spans]\n",
    "    \n",
    "    # Format the matches as a (doc.text, entities) tuple\n",
    "    training_example = (doc.text, {\"entities\": entities})\n",
    "    \n",
    "    # Append the example to the TRAINING DATA\n",
    "    TRAINING_DATA.append(training_example)\n",
    "    \n",
    "# print the training data\n",
    "print(*TRAINING_DATA, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Whoo-hoo!** training data is created as seen. Training data is usually created by humans who assign labels to texts\n",
    "- Before training a model with the data, always double-check that the matcher didn't identify any false positives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Training Loop\n",
    "   - spaCy gives full control over the training loop\n",
    "   - **Training Loop**\n",
    "      - **Loop** for a number of times (epoch).\n",
    "      - **shuffle** the training data.\n",
    "      - **Divide** the data into batches (mini-batch).\n",
    "      - **Update** the model for each batch.\n",
    "      - **save** the updated model.\n",
    "   - **Here's an example**\n",
    "      - Let's imagine we have a list of training examples consisting of texts and entity annotations. \n",
    "      - We want to loop for 10 iterations, so we're iterating over a **range** of 10.\n",
    "      - Next, we use the **random** module to randomly shuffle the training data.\n",
    "      - We then use spaCy's **minibatch** utility function to divide the examples into batches.\n",
    "      - For each batch, we get the texts and annotations and call the **nlp.update** method to update the model.\n",
    "      - Finally, we call the **nlp.to_disk** method to save the trained model to a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "\n",
    "TRAINING_DATA = [\n",
    "    (\"How to preorder the iPhone X\", {\"entities\": [(20, 28, \"GADGET\")]})\n",
    "    # And many more examples...\n",
    "]\n",
    "\n",
    "# Loop for 10 iterations\n",
    "for i in range(10):\n",
    "    # shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA):\n",
    "\n",
    "        # Split the batch in texts and annotations\n",
    "        texts = [text for text, annotation in batch]\n",
    "        annotations = [annotation for text, annotation in batch]\n",
    "\n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations)\n",
    "        \n",
    "# Save the model\n",
    "# nlp.to_disk(path_to_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**setting up a new pipeline from scratch**\n",
    "   - In this example, we start off with a blank English model using the **spacy.blank** method. The blank model doesn't have any pipeline components, only the language data and tokenization rules.\n",
    "   - We then create a blank entity recognizer and add it to the pipeline.\n",
    "   - Using the **add_label** method, we can add new string labels to the model.\n",
    "   - We can now call **nlp.begin_training** to initialize the model with random weights.\n",
    "   - To get better accuracy, we want to loop over the examples more than once and randomly shuffle the data on each iteration.\n",
    "   - On each iteration, we divide the examples into batches using spaCy's **minibatch** utility function. Each example consists of a text and its annotations.\n",
    "   - Finally, we update the model with the texts and annotations and continue the loop.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 33.13037347793579}\n",
      "{'ner': 20.98947110772133}\n",
      "{'ner': 7.492994154803455}\n",
      "{'ner': 5.156381610184326}\n",
      "{'ner': 16.641603469499387}\n",
      "{'ner': 7.548577504443529}\n",
      "{'ner': 3.697585553745739}\n",
      "{'ner': 2.508725864211314}\n",
      "{'ner': 1.599127044490345}\n",
      "{'ner': 0.8498655754095612}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# examples\n",
    "examples = [['How to preorder the iPhone X', {'entities': [[20, 28, 'GADGET']]}], \n",
    "            ['iPhone X is coming', {'entities': [[0, 8, 'GADGET']]}], \n",
    "            ['Should I pay $1,000 for the iPhone X?', {'entities': [[28, 36, 'GADGET']]}], \n",
    "            ['The iPhone 8 reviews are here', {'entities': [[4, 12, 'GADGET']]}], \n",
    "            ['Your iPhone goes up to 11 today', {'entities': [[5, 11, 'GADGET']]}], \n",
    "            ['I need a new phone! Any tips?', {'entities': []}]]\n",
    "\n",
    "# start with the blank english model\n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# create blank entity recognizer and add it to the pipeline\n",
    "ner = nlp.create_pipe(\"ner\")\n",
    "nlp.add_pipe(ner)\n",
    "\n",
    "# Add a new label\n",
    "ner.add_label(\"GADGET\")\n",
    "\n",
    "# start the training, initialzes the model with random weights\n",
    "nlp.begin_training()\n",
    "\n",
    "# train for 10 iterations\n",
    "for itn in range(10):\n",
    "    random.shuffle(examples)\n",
    "    losses = {}\n",
    "    # Divide the examples into batches, with batch_size=2\n",
    "    for batch in spacy.util.minibatch(examples, size=2):\n",
    "        texts = [text for text, annotation in batch]\n",
    "        annotations = [annotation for text, annotation in batch]\n",
    "        \n",
    "        # update the model\n",
    "        nlp.update(texts, annotations, losses=losses)\n",
    "    print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "224390953ffe4ffc953d2425899b26f28a6cfac249dfac765b546800b74315c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
