{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a95ff480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: spacy in /home/bish/.local/lib/python3.10/site-packages (3.4.4)\n",
      "Collecting spacy\n",
      "  Downloading spacy-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m0m\n",
      "\u001b[?25hRequirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /home/bish/.local/lib/python3.10/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /home/bish/.local/lib/python3.10/site-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/bish/.local/lib/python3.10/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/bish/.local/lib/python3.10/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/bish/.local/lib/python3.10/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /home/bish/.local/lib/python3.10/site-packages (from spacy) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /home/bish/.local/lib/python3.10/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /home/bish/.local/lib/python3.10/site-packages (from spacy) (2.4.6)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /home/bish/.local/lib/python3.10/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /home/bish/.local/lib/python3.10/site-packages (from spacy) (0.7.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /home/bish/.local/lib/python3.10/site-packages (from spacy) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /home/bish/.local/lib/python3.10/site-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/bish/.local/lib/python3.10/site-packages (from spacy) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/bish/.local/lib/python3.10/site-packages (from spacy) (1.24.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/lib/python3/dist-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /home/bish/.local/lib/python3.10/site-packages (from spacy) (1.10.2)\n",
      "Requirement already satisfied: jinja2 in /home/bish/.local/lib/python3.10/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy) (59.6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/bish/.local/lib/python3.10/site-packages (from spacy) (20.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /home/bish/.local/lib/python3.10/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/lib/python3/dist-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /home/bish/.local/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.6.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /home/bish/.local/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /home/bish/.local/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/lib/python3/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/bish/.local/lib/python3.10/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Installing collected packages: spacy\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.4.4\n",
      "    Uninstalling spacy-3.4.4:\n",
      "      Successfully uninstalled spacy-3.4.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "en-core-web-md 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 3.6.1 which is incompatible.\n",
      "en-core-web-sm 3.4.1 requires spacy<3.5.0,>=3.4.0, but you have spacy 3.6.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed spacy-3.6.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n",
      "/bin/bash: line 1: python: command not found\n"
     ]
    }
   ],
   "source": [
    "#installation\n",
    "!pip install -U spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0345c34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f54ad426",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        # loading english language model of spacy\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "        \n",
    "    def lower_casing(self, text):\n",
    "        \"\"\"\n",
    "        Accepts text as arguments and return text in lowercase\n",
    "\n",
    "        Arguments:\n",
    "        text: raw text\n",
    "\n",
    "        Returns:\n",
    "        text_to_lower: text converted to lower case\n",
    "        \"\"\"\n",
    "        text_to_lower = text.lower()\n",
    "\n",
    "        return text_to_lower\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        \"\"\"\n",
    "        Removes stopwords passed from the text passed as an arguments\n",
    "\n",
    "        Arguments:\n",
    "        text: raw text from where stopwords need to removed\n",
    "\n",
    "        Returns:\n",
    "        tokens_without_sw: concatenated tokens of raw text without stopwords\n",
    "        \"\"\"\n",
    "        # getting list of default stop words in spaCy english model\n",
    "        stopwords = self.nlp.Defaults.stop_words\n",
    "\n",
    "        # tokenize text\n",
    "        text_tokens = word_tokenize(text)\n",
    "\n",
    "        # remove stop words:\n",
    "        tokens_without_sw = \" \".join([word for word in text_tokens if word not in stopwords])\n",
    "\n",
    "        # return list of tokens with no stop words\n",
    "        return tokens_without_sw\n",
    "    \n",
    "    def tokenize_word(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize the text passed as an arguments into a list of words(tokens)\n",
    "\n",
    "        Arguments:\n",
    "        text: raw text\n",
    "\n",
    "        Returns:\n",
    "        words: list containing tokens in text\n",
    "        \"\"\"\n",
    "        # passing the text to nlp and initialize an object called 'doc'\n",
    "        doc = self.nlp(text)\n",
    "\n",
    "        # Tokenize the doc using token.text attribute\n",
    "        words = [token.text for token in doc]\n",
    "\n",
    "        # return list of tokens\n",
    "        return words\n",
    "    \n",
    "    def tokenize_sentence(self, text):\n",
    "        \"\"\"\n",
    "        Tokenize the text passed as an arguments into a list of sentence\n",
    "\n",
    "        Arguments:\n",
    "        text: raw text\n",
    "\n",
    "        Returns:\n",
    "        sentences: list of sentences\n",
    "        \"\"\"\n",
    "        # passing the text to nlp and initialize an object called 'doc'\n",
    "        doc = self.nlp(text)\n",
    "\n",
    "        # tokenize the sentence using sents attributes\n",
    "        sentences = list(doc.sents)\n",
    "\n",
    "        # return tokenize sentence\n",
    "        return sentences\n",
    "    \n",
    "    def remove_punctuation(self, text):\n",
    "        \"\"\"\n",
    "        removes punctuation symbols present in the raw text passed as an arguments\n",
    "\n",
    "        Arguments:\n",
    "        text: raw text\n",
    "\n",
    "        Returns: \n",
    "        not_punctuation: text without punctuation\n",
    "        \"\"\"\n",
    "        # passing the text to nlp and initialize an object called 'doc'\n",
    "        doc = self.nlp(text)\n",
    "\n",
    "        not_punctuation = []\n",
    "        # remove the puctuation\n",
    "        for token in doc:\n",
    "            if token.is_punct == False:\n",
    "                not_punctuation.append(token)\n",
    "        \n",
    "        return \" \".join([str(w) for w in not_punctuation])\n",
    "    \n",
    "    \n",
    "    def lemmatization(self, text):\n",
    "        \"\"\"\n",
    "        obtain the lemma of the each token in the text, append to the list, and returns the list\n",
    "\n",
    "        Arguments:\n",
    "        text: raw text\n",
    "\n",
    "        Returns:\n",
    "        token_lemma_list: list containing token with its lemma\n",
    "        \"\"\"\n",
    "\n",
    "        # passing the text to nlp and initialize an object called 'doc'\n",
    "        doc = self.nlp(text)\n",
    "\n",
    "        token_lemma_list = []\n",
    "        # Lemmatization\n",
    "        for token in doc:\n",
    "            token_lemma_list.append((token.text, token.lemma_))\n",
    "\n",
    "        return token_lemma_list\n",
    "    \n",
    "    def pos_tagging(self, text):\n",
    "        # passing the text to nlp and initialize an object called 'doc'\n",
    "        doc = self.nlp(text)\n",
    "\n",
    "        pos_list = []\n",
    "        for token in doc:\n",
    "            pos_list.append((token.text, token.pos_, token.tag_))\n",
    "        return pos_list\n",
    "    \n",
    "    def named_entity_recognition(self, text):\n",
    "        \"\"\"\n",
    "        returns entity_text and entity labels as a tuple\n",
    "\n",
    "        Arguments:\n",
    "        text: raw text\n",
    "\n",
    "        Returns:\n",
    "        entity_text_label: entity text and labels as a tuple\n",
    "        \"\"\"\n",
    "        # passing the text to nlp and initialize an object called 'doc'\n",
    "        doc = self.nlp(text)\n",
    "\n",
    "        #named entity recogniton using doc.ents\n",
    "        entity_text_label = []\n",
    "\n",
    "        for entity in doc.ents:\n",
    "            entity_text_label.append((entity.text, entity.label_))\n",
    "\n",
    "        return entity_text_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "adc23414",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    sample_text = \"Books, are on the table. I want to read a book ? \"\n",
    "    preprocessor = TextPreprocessor()\n",
    "    lower_text = preprocessor.lower_casing(sample_text)\n",
    "    print(lower_text)\n",
    "    remove_stopword = preprocessor.remove_stopwords(lower_text)\n",
    "    print(remove_stopword)\n",
    "    token = preprocessor.tokenize_word(remove_stopword)\n",
    "    print(token)\n",
    "    sample_text2 =  \"Oh man, this is pretty cool. We will do more such things.\"\n",
    "    sentence = preprocessor.tokenize_sentence(sample_text2)\n",
    "    print(sentence)\n",
    "    not_punctuation = preprocessor.remove_punctuation(remove_stopword)\n",
    "    print(not_punctuation)\n",
    "    lemma = preprocessor.lemmatization(not_punctuation)\n",
    "    print(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d574d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "books, are on the table. i want to read a book ? \n",
      "books , table . want read book ?\n",
      "['books', ',', 'table', '.', 'want', 'read', 'book', '?']\n",
      "[Oh man, this is pretty cool., We will do more such things.]\n",
      "books table want read book\n",
      "[('books', 'book'), ('table', 'table'), ('want', 'want'), ('read', 'read'), ('book', 'book')]\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9244207",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
