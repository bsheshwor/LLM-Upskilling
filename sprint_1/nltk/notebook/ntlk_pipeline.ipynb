{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5a4a7b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m636.8/636.8 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /home/bish/.local/lib/python3.10/site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk>=3.1->textblob) (8.0.3)\n",
      "Requirement already satisfied: joblib in /home/bish/.local/lib/python3.10/site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/bish/.local/lib/python3.10/site-packages (from nltk>=3.1->textblob) (2022.10.31)\n",
      "Requirement already satisfied: tqdm in /home/bish/.local/lib/python3.10/site-packages (from nltk>=3.1->textblob) (4.65.0)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b3b6ac",
   "metadata": {},
   "source": [
    "\n",
    "### Import Statements\n",
    "- `re`: The `re` module is imported for regular expression operations. It is used to perform pattern matching and substitutions, which can be useful for tasks like HTML tag removal and URL detection.\n",
    "- `Counter` from `collections`: The `Counter` class from the `collections` module is imported to facilitate the counting of word occurrences. It is used in operations like frequent word removal and rare word removal.\n",
    "- `ABC` and `abstractmethod` from `abc`: The `ABC` (Abstract Base Class) and `abstractmethod` are imported to define abstract classes and methods. These are used to create a common interface for text preprocessing classes.\n",
    "- `Union` and `List` from `typing`: The `Union` and `List` types from the `typing` module are imported to provide type hinting for method parameters and return values, improving code readability and maintainability.\n",
    "- `string`: The `string` module is imported for easy access to string-related constants, such as punctuation characters. This is used in the punctuation removal process.\n",
    "\n",
    "### NLTK Library\n",
    "- `nltk`: The `nltk` library, short for the Natural Language Toolkit, is imported to leverage its extensive set of tools and resources for natural language processing (NLP). NLTK provides functionalities for tokenization, stopwords, wordnet, and more.\n",
    "\n",
    "### NLTK Stopwords and WordNet\n",
    "- `stopwords` and `wordnet` from `nltk.corpus`: The `stopwords` and `wordnet` modules from `nltk.corpus` are imported to access NLTK's predefined lists of stopwords (commonly used words to be filtered out) and WordNet data (a lexical database for English).\n",
    "\n",
    "### NLTK Lemmatization and Stemming\n",
    "- `WordNetLemmatizer` and `PorterStemmer` from `nltk.stem`: The `WordNetLemmatizer` and `PorterStemmer` classes from `nltk.stem` are imported to perform lemmatization and stemming, respectively, on word tokens. Lemmatization reduces words to their base or dictionary form, while stemming reduces words to their root form.\n",
    "\n",
    "### NLTK Tokenization\n",
    "- `word_tokenize` from `nltk.tokenize`: The `word_tokenize` function from `nltk.tokenize` is imported for word tokenization. It splits text into individual words or tokens, which is a fundamental step in many text preprocessing tasks.\n",
    "\n",
    "### NLTK Sentence Tokenization\n",
    "- `sent_tokenize` from `nltk.tokenize`: The `sent_tokenize` function from `nltk.tokenize` is imported for sentence tokenization. It divides text into individual sentences, allowing for more granular analysis and processing.\n",
    "\n",
    "### TextBlob for Spelling Correction\n",
    "- `Word` from `textblob`: The `Word` class from the `textblob` library is imported to perform spelling correction. It can correct common spelling errors in text data, enhancing text quality and readability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d7415d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Union, List\n",
    "import string\n",
    "\n",
    "#importing nltk library\n",
    "import nltk\n",
    "\n",
    "#importing nltk libraries for stopwords and wordnet\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "\n",
    "#importing nltk libraries for lemmatization and stemming\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "#importing nltk libraries for word tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#importing nltk libraries for sentence tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "#importing textblob for spelling correction\n",
    "from textblob import Word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb695059",
   "metadata": {},
   "source": [
    "# Create a different classes for different text preprocessing\n",
    "\n",
    "## Text Preprocessing Code Documentation\n",
    "\n",
    "### Introduction\n",
    "This documentation provides an overview of a Python code module for text preprocessing. The code defines several classes and methods for common text preprocessing tasks using the Natural Language Toolkit (NLTK) and other libraries.\n",
    "\n",
    "### `BasePreprocessor` Abstract Class\n",
    "- `BasePreprocessor(ABC)` is an abstract base class that defines a template for text preprocessing classes.\n",
    "- The `preprocessor` method, marked as an abstract method using `@abstractmethod`, expects a text input and returns either a string or a list of strings.\n",
    "- This class serves as a blueprint for various text preprocessing operations.\n",
    "\n",
    "### `DownloadRequirement` Class\n",
    "- `DownloadRequirement` class manages the download of NLTK resources required for text preprocessing.\n",
    "- The `download` method downloads necessary NLTK resources like stopwords and WordNet data.\n",
    "\n",
    "### Preprocessing Classes\n",
    "- Several classes are defined for specific text preprocessing operations, such as lowercasing, HTML tag removal, URL removal, tokenization, stopword removal, punctuation removal, frequent word removal, rare word removal, spelling correction, lemmatization, and stemming.\n",
    "- Each class has a `preprocessor` method that takes input text and performs the respective preprocessing operation.\n",
    "- Type hints and docstrings are provided for each method to describe their purpose, expected input, and output types.\n",
    "\n",
    "### Usage\n",
    "- To use this text preprocessing module, create an instance of the `TextPreprocessor` class, providing the text you want to preprocess.\n",
    "- Call the various methods on the `TextPreprocessor` instance to perform specific preprocessing tasks in a sequence.\n",
    "- The results of each preprocessing step are stored in instance variables and can be accessed as needed.\n",
    "\n",
    "### Example Usage\n",
    "```python\n",
    "text = \"Sample text with <b>HTML tags</b> and URLs: http://example.com\"\n",
    "preprocessor = TextPreprocessor(text)\n",
    "preprocessor.lower_case()\n",
    "preprocessor.remove_tag()\n",
    "preprocessor.remove_url()\n",
    "tokens = preprocessor.tokenize_word()\n",
    "preprocessor.remove_punctuation()\n",
    "preprocessor.remove_frequent_word()\n",
    "preprocessor.remove_rare_word()\n",
    "preprocessor.correct_spelling()\n",
    "preprocessor.lemmatizer()\n",
    "preprocessor.stemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe5df208",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasePreprocessor(ABC):\n",
    "    @abstractmethod\n",
    "    def preprocessor(self, text: str) -> Union[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Abstract method for text preprocessing.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text to be preprocessed.\n",
    "\n",
    "        Returns:\n",
    "            Union[str, List[str]]: Processed text or list of processed tokens.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "class DownloadRequirement:\n",
    "    def download(self):\n",
    "        \"\"\"\n",
    "        Downloads NLTK resources for text preprocessing.\n",
    "        \"\"\"\n",
    "        download()\n",
    "        download('stopwords')\n",
    "        download('wordnet')\n",
    "\n",
    "class LowerCase(BasePreprocessor):\n",
    "    def preprocessor(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Converts the input text to lowercase.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "            str: Lowercased text.\n",
    "        \"\"\"\n",
    "        return text.lower()\n",
    "\n",
    "class RemoveTag(BasePreprocessor):\n",
    "    def preprocessor(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Removes HTML tags from the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text containing HTML tags.\n",
    "\n",
    "        Returns:\n",
    "            str: Text with HTML tags removed.\n",
    "        \"\"\"\n",
    "        regex = re.compile(r'<[^>]+>')\n",
    "        return regex.sub('', text)\n",
    "\n",
    "class RemoveURL(BasePreprocessor):\n",
    "    def preprocessor(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Removes URLs from the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text containing URLs.\n",
    "\n",
    "        Returns:\n",
    "            str: Text with URLs removed.\n",
    "        \"\"\"\n",
    "        url_search = re.search('http://\\S+|https://\\S+', text)\n",
    "        if url_search:\n",
    "            url_group = url_search.group(0)\n",
    "            return text.replace(url_group, '')\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "class TokenizeWord(BasePreprocessor):\n",
    "    def preprocessor(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokenizes the input text into words.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: List of word tokens.\n",
    "        \"\"\"\n",
    "        return word_tokenize(text)\n",
    "\n",
    "class TokenizeSentence(BasePreprocessor):\n",
    "    def preprocessor(self, text: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokenizes the input text into sentences.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: List of sentence tokens.\n",
    "        \"\"\"\n",
    "        return sent_tokenize(text)\n",
    "\n",
    "class RemoveStopword(BasePreprocessor):\n",
    "    def preprocessor(self, text: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Removes stopwords from a list of word tokens.\n",
    "\n",
    "        Args:\n",
    "            text (List[str]): List of word tokens.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: List of word tokens with stopwords removed.\n",
    "        \"\"\"\n",
    "        words = [w for w in text if w not in stopwords.words(\"english\")]\n",
    "        return words\n",
    "\n",
    "class RemovePunctuation(BasePreprocessor):\n",
    "    def preprocessor(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Removes punctuation from the input text.\n",
    "\n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "\n",
    "        Returns:\n",
    "            str: Text with punctuation removed.\n",
    "        \"\"\"\n",
    "        pun_string = text.translate(str.maketrans('', '', string.punctuation))\n",
    "        return pun_string\n",
    "\n",
    "class RemoveFrequentWord(BasePreprocessor):\n",
    "    def preprocessor(self, text: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Removes the most frequent words from a list of word tokens.\n",
    "\n",
    "        Args:\n",
    "            text (List[str]): List of word tokens.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: List of word tokens with frequent words removed.\n",
    "        \"\"\"\n",
    "        cnt = Counter()\n",
    "        for word in text:\n",
    "            cnt[word] += 1\n",
    "        FREQWORD = set([w for (w, wc) in cnt.most_common(10)])\n",
    "        return [word for word in text if word not in FREQWORD]\n",
    "\n",
    "class RemoveRareWord(BasePreprocessor):\n",
    "    def preprocessor(self, text: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Removes the rarest words from a list of word tokens.\n",
    "\n",
    "        Args:\n",
    "            text (List[str]): List of word tokens.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: List of word tokens with rare words removed.\n",
    "        \"\"\"\n",
    "        cnt = Counter()\n",
    "        for word in text:\n",
    "            cnt[word] += 1\n",
    "        n_rare_words = 10\n",
    "        RAREWORD = set([w for (w, wc) in cnt.most_common()[:-n_rare_words-1:-1]])\n",
    "        return [word for word in text if word not in RAREWORD]\n",
    "\n",
    "class CorrectSpelling(BasePreprocessor):\n",
    "    def preprocessor(self, text: List[str]) -> List[str]:\n",
    "        \"\"\"\n",
    "        Corrects the spelling of words in a list of word tokens.\n",
    "\n",
    "        Args:\n",
    "            text (List[str]): List of word tokens.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: List of word tokens with corrected spelling.\n",
    "        \"\"\"\n",
    "        temp = []\n",
    "        for word in text:\n",
    "            word = Word(word)\n",
    "            result = word.correct()\n",
    "            temp.append(result)\n",
    "        return temp\n",
    "\n",
    "class Lemmatizer(BasePreprocessor):\n",
    "    def preprocessor(self, text: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Lemmatizes a list of word tokens.\n",
    "\n",
    "        Args:\n",
    "            text (List[str]): List of word tokens.\n",
    "\n",
    "        Returns:\n",
    "            str: Text with lemmatized words.\n",
    "        \"\"\"\n",
    "        lemmed = \" \".join([WordNetLemmatizer().lemmatize(w) for w in text])\n",
    "        return lemmed\n",
    "\n",
    "class Stemmer(BasePreprocessor):\n",
    "    def preprocessor(self, text: List[str]) -> str:\n",
    "        \"\"\"\n",
    "        Stems a list of word tokens.\n",
    "\n",
    "        Args:\n",
    "            text (List[str]): List of word tokens.\n",
    "\n",
    "        Returns:\n",
    "            str: Text with stemmed words.\n",
    "        \"\"\"\n",
    "        stemmed = \" \".join([PorterStemmer().stem(w) for w in text])\n",
    "        return stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1c3b305",
   "metadata": {},
   "source": [
    "## Text Preprocessor Class Documentation\n",
    "\n",
    "\n",
    "### Class Initialization\n",
    "- `__init__(self, text: str)`: The constructor initializes a `TextPreprocessor` instance with the input text to be preprocessed.\n",
    "  - Args:\n",
    "    - `text (str)`: The input text to be preprocessed.\n",
    "\n",
    "### Class Attributes\n",
    "- The class contains several instance attributes, each representing a stage of text preprocessing. Variable names have been updated for clarity.\n",
    "  - `self.lower_text: str`: Stores the lowercase version of the input text.\n",
    "  - `self.removed_tags: str`: Stores the text with HTML tags removed.\n",
    "  - `self.removed_urls: str`: Stores the text with URLs removed.\n",
    "  - `self.tokenized_words: List[str]`: Stores a list of word tokens.\n",
    "  - `self.tokenized: List[str]`: Stores a list of tokens after punctuation removal.\n",
    "  - `self.removed_punctuation: str`: Stores text with punctuation removed.\n",
    "  - `self.removed_frequent: List[str]`: Stores a list of words with frequent words removed.\n",
    "  - `self.removed_rare: List[str]`: Stores a list of words with rare words removed.\n",
    "  - `self.spell_corrected: List[str]`: Stores a list of words with corrected spelling.\n",
    "  - `self.lemma: str`: Stores text with lemmatized words.\n",
    "  - `self.stem: str`: Stores text with stemmed words.\n",
    "\n",
    "### Text Preprocessing Methods\n",
    "- The class defines several methods for performing specific text preprocessing tasks. Each method returns the result of the operation and updates the corresponding instance attribute.\n",
    "- Method names, docstrings, and return types are provided for clarity.\n",
    "  - `lower_case(self) -> str`: Converts the input text to lowercase and returns the lowercase text.\n",
    "  - `remove_tag(self) -> str`: Removes HTML tags from the text and returns the cleaned text.\n",
    "  - `remove_url(self) -> str`: Removes URLs from the text and returns the cleaned text.\n",
    "  - `tokenize_word(self) -> List[str]`: Tokenizes the text into words and returns a list of word tokens.\n",
    "  - `remove_punctuation(self) -> str`: Removes punctuation from the text and returns the cleaned text.\n",
    "  - `remove_frequent_word(self) -> List[str]`: Removes frequent words from the text and returns a list of cleaned words.\n",
    "  - `remove_rare_word(self) -> List[str]`: Removes rare words from the text and returns a list of cleaned words.\n",
    "  - `correct_spelling(self) -> List[str]`: Corrects spelling errors in the text and returns a list of corrected words.\n",
    "  - `lemmatizer(self) -> str`: Lemmatizes the text and returns the lemmatized text.\n",
    "  - `stemmer(self) -> str`: Stems the text and returns the stemmed text.\n",
    "\n",
    "### Usage Example\n",
    "```python\n",
    "text = \"Sample text with <b>HTML tags</b> and URLs: http://example.com\"\n",
    "preprocessor = TextPreprocessor(text)\n",
    "preprocessor.lower_case()\n",
    "preprocessor.remove_tag()\n",
    "preprocessor.remove_url()\n",
    "tokens = preprocessor.tokenize_word()\n",
    "preprocessor.remove_punctuation()\n",
    "preprocessor.remove_frequent_word()\n",
    "preprocessor.remove_rare_word()\n",
    "preprocessor.correct_spelling()\n",
    "preprocessor.lemmatizer()\n",
    "preprocessor.stemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb116c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self, text: str):\n",
    "        \"\"\"\n",
    "        Initializes the TextPreprocessor instance.\n",
    "\n",
    "        Args:\n",
    "            text (str): The input text to be preprocessed.\n",
    "        \"\"\"\n",
    "        self.text = text\n",
    "        self.download_requirement = DownloadRequirement()\n",
    "        self.download_requirement.download()\n",
    "        self.lower_text: str = \"\"\n",
    "        self.removed_tags: str = \"\"  # Variable name changed from 'self.removed_tags' to 'self.removed_tags'\n",
    "        self.removed_urls: str = \"\"  # Variable name changed from 'self.removed_urls' to 'self.removed_urls'\n",
    "        self.tokenized_words: List[str] = []  # Variable name changed from 'self.tokenized_words' to 'self.tokenized_words'\n",
    "        self.tokenized: List[str] = []  # Variable name changed from 'self.tokenized' to 'self.tokenized'\n",
    "        self.removed_punctuation: str = \"\"  # Variable name changed from 'self.removed_punctuation' to 'self.removed_punctuation'\n",
    "        self.removed_frequent: List[str] = []\n",
    "        self.removed_rare: List[str] = []\n",
    "        self.spell_corrected: List[str] = []  # Variable name changed from 'self.spell_corrected' to 'self.spell_corrected'\n",
    "        self.lemma: str = \"\"\n",
    "        self.stem: str = \"\"\n",
    "    \n",
    "    def lower_case(self) -> str:\n",
    "        \"\"\"\n",
    "        Converts the input text to lowercase.\n",
    "\n",
    "        Returns:\n",
    "            str: The input text in lowercase.\n",
    "        \"\"\"\n",
    "        text_processor = LowerCase()\n",
    "        self.lower_text = text_processor.preprocessor(self.text)\n",
    "        return self.lower_text\n",
    "    \n",
    "    def remove_tag(self) -> str:\n",
    "        \"\"\"\n",
    "        Removes HTML tags from the text.\n",
    "\n",
    "        Returns:\n",
    "            str: Text with HTML tags removed.\n",
    "        \"\"\"\n",
    "        text_processor = RemoveTag()\n",
    "        self.removed_tags = text_processor.preprocessor(self.lower_text)\n",
    "        return self.removed_tags\n",
    "\n",
    "    def remove_url(self) -> str:\n",
    "        \"\"\"\n",
    "        Removes URLs from the text.\n",
    "\n",
    "        Returns:\n",
    "            str: Text with URLs removed.\n",
    "        \"\"\"\n",
    "        text_processor = RemoveURL()\n",
    "        self.removed_urls = text_processor.preprocessor(self.removed_tags)\n",
    "        return self.removed_urls\n",
    "    \n",
    "    def tokenize_word(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Tokenizes the text into words.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: List of word tokens.\n",
    "        \"\"\"\n",
    "        text_processor = TokenizeWord()\n",
    "        self.tokenized_words = text_processor.preprocessor(self.removed_urls)\n",
    "        return self.tokenized_words\n",
    "    \n",
    "    def remove_punctuation(self) -> str:\n",
    "        \"\"\"\n",
    "        Removes punctuation from the text.\n",
    "\n",
    "        Returns:\n",
    "            str: Text with punctuation removed.\n",
    "        \"\"\"\n",
    "        self.tokenized_words = \" \".join(self.tokenized_words)\n",
    "        text_processor = RemovePunctuation()\n",
    "        self.removed_punctuation = text_processor.preprocessor(self.tokenized_words)\n",
    "        return self.removed_punctuation\n",
    "    \n",
    "    def remove_frequent_word(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Removes the most frequent words from the text.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: List of words with frequent words removed.\n",
    "        \"\"\"\n",
    "        text_processor = TokenizeWord()\n",
    "        self.tokenized = text_processor.preprocessor(self.removed_punctuation)\n",
    "        text_processor = RemoveFrequentWord()\n",
    "        self.removed_frequent = text_processor.preprocessor(self.tokenized)\n",
    "        return self.removed_frequent\n",
    "    \n",
    "    def remove_rare_word(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Removes the rarest words from the text.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: List of words with rare words removed.\n",
    "        \"\"\"\n",
    "        text_processor = RemoveRareWord()\n",
    "        self.removed_rare = text_processor.preprocessor(self.tokenized)\n",
    "        return self.removed_rare\n",
    "    \n",
    "    def correct_spelling(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Corrects the spelling of words in the text.\n",
    "\n",
    "        Returns:\n",
    "            List[str]: List of words with corrected spelling.\n",
    "        \"\"\"\n",
    "        text_processor = CorrectSpelling()\n",
    "        self.spell_corrected = text_processor.preprocessor(self.removed_frequent)\n",
    "        return self.spell_corrected\n",
    "    \n",
    "    def lemmatizer(self) -> str:\n",
    "        \"\"\"\n",
    "        Lemmatizes the text.\n",
    "\n",
    "        Returns:\n",
    "            str: Text with lemmatized words.\n",
    "        \"\"\"\n",
    "        text_processor = Lemmatizer()\n",
    "        self.lemma = text_processor.preprocessor(self.spell_corrected)\n",
    "        return self.lemma\n",
    "    \n",
    "    def stemmer(self) -> str:\n",
    "        \"\"\"\n",
    "        Stems the text.\n",
    "\n",
    "        Returns:\n",
    "            str: Text with stemmed words.\n",
    "        \"\"\"\n",
    "        text_processor = Stemmer()\n",
    "        self.stem = text_processor.preprocessor(self.spell_corrected)\n",
    "        return self.stem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a91bb7",
   "metadata": {},
   "source": [
    "## Test the above code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e82f3795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/bish/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/bish/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html> @bshesh dr. smith graduated from the university of washington. he later started an analytics firm called lux, which catered to enterprise customers. https://www.bisheshwor.com.np\n",
      " @bshesh dr. smith graduated from the university of washington. he later started an analytics firm called lux, which catered to enterprise customers. https://www.bisheshwor.com.np\n",
      " @bshesh dr. smith graduated from the university of washington. he later started an analytics firm called lux, which catered to enterprise customers. \n",
      "['@', 'bshesh', 'dr.', 'smith', 'graduated', 'from', 'the', 'university', 'of', 'washington', '.', 'he', 'later', 'started', 'an', 'analytics', 'firm', 'called', 'lux', ',', 'which', 'catered', 'to', 'enterprise', 'customers', '.']\n",
      " bshesh dr smith graduated from the university of washington  he later started an analytics firm called lux  which catered to enterprise customers \n",
      "['later', 'started', 'an', 'analytics', 'firm', 'called', 'lux', 'which', 'catered', 'to', 'enterprise', 'customers']\n",
      "['bshesh', 'dr', 'smith', 'graduated', 'from', 'the', 'university', 'of', 'washington', 'he', 'later', 'started']\n",
      "['later', 'started', 'an', 'analysis', 'firm', 'called', 'klux', 'which', 'watered', 'to', 'enterprise', 'customers']\n",
      "later started an analysis firm called klux which watered to enterprise customer\n",
      "later start an analysi firm call klux which water to enterpris custom\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    text = \"<html> @bshesh Dr. Smith graduated from the University of Washington. He later started an analytics firm called Lux, which catered to enterprise customers. https://www.bisheshwor.com.np\"\n",
    "    text_preprocessor = TextPreprocessor(text)\n",
    "    print(text_preprocessor.lower_case())\n",
    "    print(text_preprocessor.remove_tag())\n",
    "    print(text_preprocessor.remove_url())\n",
    "    print(text_preprocessor.tokenize_word())\n",
    "    print(text_preprocessor.remove_punctuation())\n",
    "    print(text_preprocessor.remove_frequent_word())\n",
    "    print(text_preprocessor.remove_rare_word())\n",
    "    print(text_preprocessor.correct_spelling())\n",
    "    print(text_preprocessor.lemmatizer())\n",
    "    print(text_preprocessor.stemmer())\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca445a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html> @bshesh dr. smith graduated from the university of washington. he later started an analytics firm called lux, which catered to enterprise customers. https://www.bisheshwor.com.np\n"
     ]
    }
   ],
   "source": [
    "text = \"<html> @bshesh Dr. Smith graduated from the University of Washington. He later started an analytics firm called Lux, which catered to enterprise customers. https://www.bisheshwor.com.np\"\n",
    "text_processor = LowerCase()\n",
    "lower_text = text_processor.preprocessor(text)\n",
    "print(lower_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2f47bb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " @bshesh dr. smith graduated from the university of washington. he later started an analytics firm called lux, which catered to enterprise customers. https://www.bisheshwor.com.np\n"
     ]
    }
   ],
   "source": [
    "text_processor = RemoveTag()\n",
    "remove_tag = text_processor.preprocessor(lower_text)\n",
    "print(remove_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09fe4540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " @bshesh dr. smith graduated from the university of washington. he later started an analytics firm called lux, which catered to enterprise customers. \n"
     ]
    }
   ],
   "source": [
    "text_processor = RemoveURL()\n",
    "remove_url = text_processor.preprocessor(remove_tag)\n",
    "print(remove_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73b48264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@', 'bshesh', 'dr.', 'smith', 'graduated', 'from', 'the', 'university', 'of', 'washington', '.', 'he', 'later', 'started', 'an', 'analytics', 'firm', 'called', 'lux', ',', 'which', 'catered', 'to', 'enterprise', 'customers', '.']\n"
     ]
    }
   ],
   "source": [
    "text_processor = TokenizeWord()\n",
    "tokenize_word = text_processor.preprocessor(remove_url)\n",
    "print(tokenize_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3681c2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " bshesh dr smith graduated from the university of washington  he later started an analytics firm called lux  which catered to enterprise customers \n"
     ]
    }
   ],
   "source": [
    "tokenize_word = \" \".join(tokenize_word)\n",
    "text_processor = RemovePunctuation()\n",
    "remove_punctuation = text_processor.preprocessor(tokenize_word)\n",
    "print(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92b97004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['later', 'started', 'an', 'analytics', 'firm', 'called', 'lux', 'which', 'catered', 'to', 'enterprise', 'customers']\n"
     ]
    }
   ],
   "source": [
    "text_processor = TokenizeWord()\n",
    "tokenize = text_processor.preprocessor(remove_punctuation)\n",
    "text_processor = RemoveFrequentWord()\n",
    "remove_frequent = text_processor.preprocessor(tokenize)\n",
    "print(remove_frequent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce87961f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bshesh', 'dr', 'smith', 'graduated', 'from', 'the', 'university', 'of', 'washington', 'he', 'later', 'started']\n"
     ]
    }
   ],
   "source": [
    "text_processor = RemoveRareWord()\n",
    "remove_rare = text_processor.preprocessor(tokenize)\n",
    "print(remove_rare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4f9049cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['later', 'started', 'an', 'analysis', 'firm', 'called', 'klux', 'which', 'watered', 'to', 'enterprise', 'customers']\n"
     ]
    }
   ],
   "source": [
    "text_processor = CorrectSpelling()\n",
    "spell_correct = text_processor.preprocessor(remove_frequent)\n",
    "print(spell_correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b6658b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "later started an analysis firm called klux which watered to enterprise customer\n"
     ]
    }
   ],
   "source": [
    "text_processor = Lemmatizer()\n",
    "lemma = text_processor.preprocessor(spell_correct)\n",
    "print(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d1d578ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "later start an analysi firm call klux which water to enterpris custom\n"
     ]
    }
   ],
   "source": [
    "text_processor = Stemmer()\n",
    "stem = text_processor.preprocessor(spell_correct)\n",
    "print(stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d1ee3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
