{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing  \n",
    "- Data preprocessing is an essential step in building a Machine Learning Model.\n",
    "- Depending on how well the data has been preprocessed; the results are seen after training.\n",
    "\n",
    "# Text Preprocessing in NLP  \n",
    "- Text preprocessing is the first step in the process of building a model.\n",
    "- The text is represented as a vector in multi-dimensional space.\n",
    "- In the vector space model, each word/term is an axis/dimension. The number of unique words means the number of dimensions.\n",
    "- Hence, text preprocessing steps are widely used for dimensionality reduction.\n",
    "- Various text preprocessing steps are:  \n",
    "    i.Tokenization  \n",
    "    ii.Lower casing  \n",
    "    iii.Stop words removal  \n",
    "    iv. Stemming  \n",
    "    v. Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with NLTK  \n",
    "- NLTK stands for **Natural Language Toolkit**\n",
    "- In this notebook, nltk library will be used to preprocess the text data\n",
    "- Before beginning we need to install nltk, run below cell to install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install nltk\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#browse the available packages\n",
    "# import nltk #import nltk\n",
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select required module you want install, for example **book** in this case.  \n",
    "If you want to install all module, simply click on Download. \n",
    "\n",
    "Now, lets load book module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Introductory Examples for the NLTK Book ***\n",
      "Loading text1, ..., text9 and sent1, ..., sent9\n",
      "Type the name of the text or sentence to view it.\n",
      "Type: 'texts()' or 'sents()' to list the materials.\n",
      "text1: Moby Dick by Herman Melville 1851\n",
      "text2: Sense and Sensibility by Jane Austen 1811\n",
      "text3: The Book of Genesis\n",
      "text4: Inaugural Address Corpus\n",
      "text5: Chat Corpus\n",
      "text6: Monty Python and the Holy Grail\n",
      "text7: Wall Street Journal\n",
      "text8: Personals Corpus\n",
      "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
     ]
    }
   ],
   "source": [
    "#import all from book module\n",
    "from nltk.book import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see nltk book libraries contains 9 text(raw data) which is displayed    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.Tokenization**\n",
    "- NLTK contains module called tokenize() which further classifies into two categories\n",
    "   - **Word Tokenization**\n",
    "        - Word Tokenization simply means splitting sentence/text in words.\n",
    "        - It is the process of splitting the given text into smaller pieces called tokens.\n",
    "        - Words, numbers, punctuation marks, and others can be considered as tokens.\n",
    "        - Use the **word_tokenize()** from **nltk.tokenize**\n",
    "        \n",
    "   - **Sentence Tokenization**  \n",
    "       - Sentence Tokenization is the process of splitting up strings into sentences.\n",
    "       - A sentence usually ends with a full stop (.), here focus is to study the structure of sentence in the analysis.\n",
    "       - use the **sent_tokenize()** from **nltk.tokenize**\n",
    "- Visit below for more:\n",
    "    - [tokenization_by_analyticsvidhya](https://www.analyticsvidhya.com/blog/2019/07/how-get-started-nlp-6-unique-ways-perform-tokenization/#:~:text=Tokenization%20using%20Gensim-,What%20is%20Tokenization%20in%20NLP%3F,to%20working%20with%20text%20data.&text=Tokenization%20is%20essentially%20splitting%20a,smaller%20units%20are%20called%20tokens.)\n",
    "       \n",
    "Lets start by practical example for **word tokenization,**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Founded', 'in', '2002', ',', 'SpaceX', '’', 's', 'mission', 'is', 'to', 'enable', 'humans', 'to', 'become', 'a', 'spacefaring', 'civilization', 'and', 'a', 'multi-planet', 'species', 'by', 'building', 'a', 'self-sustaining', 'city', 'on', 'Mars', '.', 'In', '2008', ',', 'SpaceX', '’', 's', 'Falcon', '1', 'became', 'the', 'first', 'privately', 'developed', 'liquid-fuel', 'launch', 'vehicle', 'to', 'orbit', 'the', 'Earth', '.']\n"
     ]
    }
   ],
   "source": [
    "#word tokenization example using NLTK\n",
    "\n",
    "#import word_tokenize from nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#define sentence\n",
    "sentence = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \n",
    "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed \n",
    "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
    "\n",
    "#tokenize words\n",
    "words = word_tokenize(sentence)\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see practical example for **sentence tokenization,**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \\nspecies by building a self-sustaining city on Mars.', 'In 2008, SpaceX’s Falcon 1 became the first privately developed \\nliquid-fuel launch vehicle to orbit the Earth.']\n"
     ]
    }
   ],
   "source": [
    "#sentence tokenization example using nltk\n",
    "\n",
    "#import sent_tokenize from nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"\"\"Founded in 2002, SpaceX’s mission is to enable humans to become a spacefaring civilization and a multi-planet \n",
    "species by building a self-sustaining city on Mars. In 2008, SpaceX’s Falcon 1 became the first privately developed \n",
    "liquid-fuel launch vehicle to orbit the Earth.\"\"\"\n",
    "\n",
    "#tokenize sentence\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.Lower Casing**  \n",
    "- Converting word to lower case (NLP->nlp).\n",
    "- **Q.Why Lower Casing?**\n",
    "    - Words like **Book** and **book** mean the same,\n",
    "    - When not converted to the lower case those two are represented as two different words in the vector space model (resulting in more dimension).\n",
    "    - Higher the dimension, more computation resources are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "books are on the table.\n"
     ]
    }
   ],
   "source": [
    "#lower casing\n",
    "sentence = \"Books are on the table.\"\n",
    "sentence = sentence.lower()\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.Stop words removal**   \n",
    "- In natural language processing, useless words (data), are referred to as **stop words**\n",
    "- stop words are very commonly used words(a, an, the, etc.) in the documents.\n",
    "- These kind of words do not signify any importance as they do not help in distinguishing two documents   \n",
    "- NLTK in python has a list of stopwords stored in 16 different languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Filtered words aafter removing the stop words from the given sentence is:**\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['NLTK',\n",
       " 'leading',\n",
       " 'platform',\n",
       " 'building',\n",
       " 'Python',\n",
       " 'programs',\n",
       " 'work',\n",
       " 'human',\n",
       " 'language',\n",
       " 'data',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stop words removal\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sentence = \"NLTK is a leading platform for building Python programs to work with human language data.\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(sentence)\n",
    "\n",
    "filtered_sentence = [word for word in word_tokens if word not in stop_words]\n",
    "\n",
    "print(\"**Filtered words aafter removing the stop words from the given sentence is:**\")\n",
    "filtered_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above result, we can see that stop words such as is, a, for, to, with are removed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.Stemming**  \n",
    "   - Stemming is a process of transforming a word to its root form.\n",
    "   - Stemming reduces the words _\"chocolates\", \"chocolatey\", \"choco\"_ to the root word, _\"chocolate\"_, and _\"retrieval\", \"retrieved\", \"retrieves\"_ to the stem _\"retrieve\"_.\n",
    "   - Stemming is an important part of the pipelining process in Natural Language Processing.\n",
    "   - Input to the Stemming is tokenized words.\n",
    "   - Result obtained after stemming i.e. stem may or may not have meaning.\n",
    "   - Some more example of Stemming for:\n",
    "       - root word _\"like\"_ include:\n",
    "           - \"likes\"\n",
    "           - \"liked\"\n",
    "           - \"likely\" \n",
    "           - \"liking\"\n",
    "       - word _\"histori\"_ include:\n",
    "           - history\n",
    "           - historical\n",
    "       - word _\"fina\"_ include:\n",
    "           - \"finally\"\n",
    "           - \"final\" \n",
    "           - \"finalized\"\n",
    "       - word _\"go\"_ include:\n",
    "           - \"going\"\n",
    "           - \"goes\"\n",
    "           - \n",
    "           \n",
    "**Errors in Stemming**  \n",
    "   - **Over stemming**\n",
    "        - It is the process where a much larger part of a word is chopped off than what is required, which in turn leads to two or more words being reduced  to the same root word or stem incorrectly when they should have been reduced to two or more stem words.\n",
    "        - Example:  \n",
    "            - _University_ and _universe_\n",
    "            - Some stemming algorithm may reduce both the words to the stem univers, which would imply both the words mean the same thing, and that is clearly wrong.  \n",
    "            - Refers to false-positives\n",
    "   - **Under stemming**  \n",
    "       - In under stemming, two or more words could be wrongly reduced to more than one root word, when they actually should be reduced to the same root word.\n",
    "       - Example: \n",
    "           - consider the words \"data\" and \"datum.\"\n",
    "           - Some algorithms may reduce these words to dat and datu respectively, which is obviously wrong. \n",
    "           - Both of these have to be reduced to the same stem dat.\n",
    "           \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machin\n",
      "learning,\n",
      "is\n",
      "cool\n"
     ]
    }
   ],
   "source": [
    "#stemming words\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "sentence = \"Machine Learning, is cool\"\n",
    "\n",
    "#tokenize\n",
    "words = sentence.split()\n",
    "\n",
    "#stemming\n",
    "for word in words:\n",
    "    print(ps.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**  \n",
    "   - The word 'machine' has its suffix 'e' chopped off.\n",
    "   - The stem does not make sense as it is not a word in English.\n",
    "   - This is a disadvantage of stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.Lemmatization**  \n",
    "   - Unlike stemmming, lemmatization reduces the words to a word existing in the language. \n",
    "   - Libraries such as nltk, spacy have stemmers and lemmatizers implemented.\n",
    "   - Example of wordnet lemmatizer is as shown.\n",
    "       - Wordnet is an large, freely and publicly available lexical database for the English language aiming to establish structured semantic(relating to meaning in language or logic) relationships between words.\n",
    "       - It is most commonly used lemmatizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatization of word **bats** is: bat\n",
      "Lemmatization of word **are** is: are\n",
      "Lemmatization of word **feet** is: foot\n"
     ]
    }
   ],
   "source": [
    "#import wordnet lemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "#Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#lemmatize single word\n",
    "print(\"Lemmatization of word **bats** is:\", lemmatizer.lemmatize('bats'))\n",
    "print(\"Lemmatization of word **are** is:\", lemmatizer.lemmatize('are'))\n",
    "print(\"Lemmatization of word **feet** is:\", lemmatizer.lemmatize('feet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The striped bat are hanging on their foot for best\n"
     ]
    }
   ],
   "source": [
    "#Lemmatize simple sentence\n",
    "import nltk\n",
    "\"\"\"\n",
    "1. First tokenize the sentence into words, nltk.word_tokenize\n",
    "2. call lemmatizer.lemmatize() on each word, can be one through list comprehension\n",
    "\"\"\"\n",
    "\n",
    "# Define the sentence to be lemmatized\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "# Tokenize: Split the sentence into words\n",
    "word_list = nltk.word_tokenize(sentence)\n",
    "\n",
    "# Lemmatize list of words and join\n",
    "lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "print(lemmatized_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Above code is simple example of how to use the wordnet lemmatizer on words and sentences.\n",
    "- Notice it didn't do a good job. Because, 'are' is not converted to 'be' and 'hanging' is not converted to 'hang' as expected\n",
    "- This can be corrected if we provide the correct (POS tag) as the second argument to **lemmatize()**.\n",
    "- Sometimes, the same word can have a multiple lemmas based on the meaning/context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strip\n",
      "stripe\n"
     ]
    }
   ],
   "source": [
    "#using pos tag\n",
    "print(lemmatizer.lemmatize(\"stripes\", \"v\"))\n",
    "\n",
    "print(lemmatizer.lemmatize(\"stripes\", \"n\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above lemmas are different based on POS tagging  \n",
    "\n",
    "**Problem:**  \n",
    "   - It may not be possible manaually provide the correct POS tag for every word for large texts.\n",
    "\n",
    "**solution:**\n",
    "   - So, instead, we will find out the correct POS tag for each word, map it to the right input character that the WordnetLemmatizer accepts and pass it as the second argument to lemmatize()\n",
    "   \n",
    "**Q. How to get POS tag for a given word?**  \n",
    "   - In nltk, it is available through the **nltk.pos_tag()** method\n",
    "   - **nltk.pos_tag()** only accepts a list of words i.e. list, even if its a single word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foot\n",
      "['The', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'best']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize with POS Tag\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "\n",
    "# 1. Init Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# 2. Lemmatize Single Word with the appropriate POS tag\n",
    "word = 'feet'\n",
    "print(lemmatizer.lemmatize(word, get_wordnet_pos(word)))\n",
    "\n",
    "# 3. Lemmatize a Sentence with the appropriate POS tag\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in nltk.word_tokenize(sentence)])\n",
    "#> ['The', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'best']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, To improve the performance of lemmatization, we need to find the part of speech for each word in given string.\n",
    "\n",
    "\n",
    "           \n",
    "**Difference between Stemming and Lemmatization**\n",
    " - Both Stemming and Lemmatization reduces the words to its base forms but,\n",
    "    - Lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meaning and spelling errors.\n",
    "    - In Lemmatization obtained base forms will make sense i.e. has meaning. Base form will be an actual words.\n",
    "    - While incase of Stemming, the obtained base words may not have always meaning, it may be some non-existing word in dictionary.\n",
    "    \n",
    "- Stemming is widely used, user can also make their own stemmer algorithm, whereas Lemmatization is not widely used, as it requires a lot of work.\n",
    "    - stemming algorithm is found a lot in different languages\n",
    "    - Lemmatization algorithm is not found much and is much more difficult and requires language knowledge as well.\n",
    "\n",
    "- Stemmer is easy to build than a lemmatizer as the latter requires deep linguistics knowledge in constructing dictionaries to look up the lemma of the word.\n",
    "\n",
    "- Example:\n",
    "    - \"Caring\" -> Lemmatization -> \"Care\"\n",
    "    - \"Caring\" -> Stemming -> \"Car\"\n",
    "\n",
    "- Hence, \n",
    "    - In Stemming base form may or may not make sense\n",
    "    - In Lemmatization base form always make sense\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.Removal of Symbols and numbers**  \n",
    "   - Text data often contains symbols and numbers which are not quite useful for the purpose of analysis\n",
    "   - Regular Expression: sub()\n",
    "       - use to replace particular pattern of string in text by another string\n",
    "       - For more about regex visit:\n",
    "           - [regex_w3_school](https://www.w3schools.com/python/python_regex.asp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "removed comments 875-873-2345\n",
      "\n",
      "string after removing hyphens: 8758732345\n"
     ]
    }
   ],
   "source": [
    "#import regular expression\n",
    "import re\n",
    "\n",
    "#example-1\n",
    "str = '875-873-2345 # commentss'\n",
    "\n",
    "#remove comments\n",
    "re_obj = re.sub(r'#.*$', '', str).strip()\n",
    "\n",
    "#show\n",
    "print(\"removed comments\", re_obj)\n",
    "\n",
    "#remove hyphens from re_obj\n",
    "#\\D returns match from string where the string doesnot contain numbers\n",
    "re_hyphen_removed = re.sub(r'\\D', '', re_obj).strip() \n",
    "print(\"\\nstring after removing hyphens:\", re_hyphen_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "United States stock-index futures pointed\n",
      "to a solidly higher open on Monday, \n",
      "indicating that major \n",
      "benchmarks were poised to United StatesA reboundfrom last week’s sharp decline, \n",
      "\n",
      "which represented their biggest weekly drops in months.\n"
     ]
    }
   ],
   "source": [
    "#example-2\n",
    "string =\"\"\"U.S. stock-index futures pointed\n",
    "to a solidly higher open on Monday, \n",
    "indicating that major \n",
    "benchmarks were poised to USA reboundfrom last week’s sharp decline, \n",
    "\\nwhich represented their biggest weekly drops in months.\"\"\"\n",
    "\n",
    "#replace \"U.S.\", \"US\", \"USA\" to \"United States\"\n",
    "# | symbol indicates or \n",
    "print(re.sub(r'U.S.|US|USA', 'United States', string))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. Named Entity Recognition**\n",
    "   - Any word which reprsents a person, organization, location etc. is a **Named Entity**.\n",
    "   - Named entity recognition is a subtask of information Extraction and is the process of identifying words which are named entites in a given text. It is also called **entity identification** or **entity chunking**.\n",
    "   - **Example:**\n",
    "       - \"Apple acquired Zoom in China on Wednesday 6th May 2020\"\n",
    "           - Here named entities are **Apple**, **Zoom**, **China**.\n",
    "           - **Named entity recognition** is the task of identifying these words from the text\n",
    "   - **Visit:** [Named Entity Recognition](https://github.com/raaga500/YTshared/blob/master/V3_NamedEntityReco_3.ipynb)\n",
    "       \n",
    "       \n",
    "   - **Q. Why is it important?**\n",
    "       - Inorder to understand the meaning from a text (for ex a tweet or document), it is important to identify who did what to whom.\n",
    "       - **Named Entity Recognition** is the first task of **identifying the words** which may represent the **who**, **what**, and **whom** in the text.\n",
    "       - It helps in identifying the major entities the text is talking about.\n",
    "       - Any NLP task which involves automatically understanding text and acts based on it, needs **Named Entity Recognition** in its pipeline.\n",
    "   \n",
    "   \n",
    "   - **Caveat**\n",
    "       - No algorithm can 100% identify all the named entities correctly.\n",
    "       \n",
    "   \n",
    "   \n",
    "   - **Approaches**\n",
    "       - **Basic NLTK algorithm**\n",
    "           - with word segmentation\n",
    "           - with sentence segmentation\n",
    "       - **Stanford NLP NER**\n",
    "       - **Using spaCy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data\n",
    "text = \"Apple acquired Zoom in China on Wednesday 6th May 2020.\\\n",
    "This news has made Apple and Google stock jump by 5% on Dow Jones Index in the \\\n",
    "United States of America\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Apple', 'acquired', 'Zoom', 'in', 'China']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize to words\n",
    "words = nltk.word_tokenize(text)\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Apple', 'NNP'),\n",
       " ('acquired', 'VBD'),\n",
       " ('Zoom', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('China', 'NNP')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# part of speech tagging\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "pos_tags[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n"
     ]
    }
   ],
   "source": [
    "# check nltk help for description of the tag\n",
    "nltk.help.upenn_tagset(\"NNP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset(\"VBD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN: preposition or conjunction, subordinating\n",
      "    astride among uppon whether out inside pro despite on by throughout\n",
      "    below within for towards near behind atop around if like until below\n",
      "    next into if beside ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset(\"IN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting svgling\n",
      "  Downloading svgling-0.3.1-py3-none-any.whl (21 kB)\n",
      "Collecting svgwrite (from svgling)\n",
      "  Downloading svgwrite-1.4.3-py3-none-any.whl (67 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m613.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m488.6 kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: svgwrite, svgling\n",
      "Successfully installed svgling-0.3.1 svgwrite-1.4.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.2.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install svgling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight:normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,1544.0,168.0\" width=\"1544px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">S</text></svg><svg width=\"3.62694%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NE</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Apple</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"1.81347%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"5.18135%\" x=\"3.62694%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">acquired</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"6.21762%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.10881%\" x=\"8.80829%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Zoom</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"10.3627%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.07254%\" x=\"11.9171%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"12.9534%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.62694%\" x=\"13.9896%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NE</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">China</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"15.8031%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.07254%\" x=\"17.6166%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">on</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"18.6528%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"5.69948%\" x=\"19.6891%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Wednesday</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"22.5389%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.59067%\" x=\"25.3886%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">6th</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.6839%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.59067%\" x=\"27.9793%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">May</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"29.2746%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"5.69948%\" x=\"30.5699%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">2020.This</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"33.4197%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.10881%\" x=\"36.2694%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">news</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"37.8238%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.59067%\" x=\"39.3782%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">has</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBZ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"40.6736%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.10881%\" x=\"41.9689%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">made</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">VBN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"43.5233%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.62694%\" x=\"45.0777%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NE</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Apple</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"46.8912%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.59067%\" x=\"48.7047%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">and</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">CC</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"4.14508%\" x=\"51.2953%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NE</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Google</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"53.3679%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.62694%\" x=\"55.4404%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">stock</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"57.2539%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.10881%\" x=\"59.0674%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">jump</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"60.6218%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.07254%\" x=\"62.1762%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">by</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"63.2124%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.07254%\" x=\"64.2487%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">5</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"65.285%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.07254%\" x=\"66.3212%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">%</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"67.3575%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.07254%\" x=\"68.3938%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">on</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"69.4301%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.59067%\" x=\"70.4663%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Dow</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"71.7617%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.62694%\" x=\"73.057%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Jones</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"74.8705%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"3.62694%\" x=\"76.6839%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">Index</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"78.4974%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.07254%\" x=\"80.3109%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"81.3472%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.59067%\" x=\"82.3834%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"83.6788%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"8.29016%\" x=\"84.9741%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NE</text></svg><svg width=\"50%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">United</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"25%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"50%\" x=\"50%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">States</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNPS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"75%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"89.1192%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"2.07254%\" x=\"93.2642%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">of</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"94.3005%\" y1=\"1.2em\" y2=\"3em\" /><svg width=\"4.66321%\" x=\"95.3368%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NE</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">America</text></svg><svg width=\"100%\" x=\"0%\" y=\"3em\"><defs /><svg width=\"100%\" x=\"0\" y=\"0em\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"1em\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"1.2em\" y2=\"3em\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"97.6684%\" y1=\"1.2em\" y2=\"3em\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [Tree('NE', [('Apple', 'NNP')]), ('acquired', 'VBD'), ('Zoom', 'NNP'), ('in', 'IN'), Tree('NE', [('China', 'NNP')]), ('on', 'IN'), ('Wednesday', 'NNP'), ('6th', 'CD'), ('May', 'NNP'), ('2020.This', 'CD'), ('news', 'NN'), ('has', 'VBZ'), ('made', 'VBN'), Tree('NE', [('Apple', 'NNP')]), ('and', 'CC'), Tree('NE', [('Google', 'NNP')]), ('stock', 'NN'), ('jump', 'NN'), ('by', 'IN'), ('5', 'CD'), ('%', 'NN'), ('on', 'IN'), ('Dow', 'NNP'), ('Jones', 'NNP'), ('Index', 'NNP'), ('in', 'IN'), ('the', 'DT'), Tree('NE', [('United', 'NNP'), ('States', 'NNPS')]), ('of', 'IN'), Tree('NE', [('America', 'NNP')])])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ne_chunk, Binary=True\n",
    "chunks = nltk.ne_chunk(pos_tags, binary=True) # either NE or not NE\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entities</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>America</td>\n",
       "      <td>NE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>China</td>\n",
       "      <td>NE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Apple</td>\n",
       "      <td>NE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>United States</td>\n",
       "      <td>NE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Google</td>\n",
       "      <td>NE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Entities Labels\n",
       "0        America     NE\n",
       "1          China     NE\n",
       "2          Apple     NE\n",
       "3  United States     NE\n",
       "4         Google     NE"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display entities in tabular format\n",
    "entities =[]\n",
    "labels =[]\n",
    "for chunk in chunks:\n",
    "    if hasattr(chunk,'label'): # it returns True if has label NE\n",
    "        #print(chunk)\n",
    "        entities.append(' '.join(c[0] for c in chunk))\n",
    "        labels.append(chunk.label())\n",
    "        \n",
    "entities_labels = list(set(zip(entities, labels)))\n",
    "entities_df = pd.DataFrame(entities_labels)\n",
    "entities_df.columns = [\"Entities\",\"Labels\"]\n",
    "entities_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(PERSON Apple/NNP)\n",
      "('acquired', 'VBD')\n",
      "(PERSON Zoom/NNP)\n",
      "('in', 'IN')\n",
      "(GPE China/NNP)\n",
      "('on', 'IN')\n",
      "('Wednesday', 'NNP')\n",
      "('6th', 'CD')\n",
      "('May', 'NNP')\n",
      "('2020.This', 'CD')\n",
      "('news', 'NN')\n",
      "('has', 'VBZ')\n",
      "('made', 'VBN')\n",
      "(PERSON Apple/NNP)\n",
      "('and', 'CC')\n",
      "(ORGANIZATION Google/NNP)\n",
      "('stock', 'NN')\n",
      "('jump', 'NN')\n",
      "('by', 'IN')\n",
      "('5', 'CD')\n",
      "('%', 'NN')\n",
      "('on', 'IN')\n",
      "(PERSON Dow/NNP Jones/NNP Index/NNP)\n",
      "('in', 'IN')\n",
      "('the', 'DT')\n",
      "(GPE United/NNP States/NNPS)\n",
      "('of', 'IN')\n",
      "(GPE America/NNP)\n"
     ]
    }
   ],
   "source": [
    "# predict entities including labels, setting binary=False\n",
    "chunks = nltk.ne_chunk(pos_tags, binary=False) #either NE or not NE\n",
    "for chunk in chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Entities</th>\n",
       "      <th>Labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>China</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dow Jones Index</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>America</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apple</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>United States</td>\n",
       "      <td>GPE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Zoom</td>\n",
       "      <td>PERSON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Google</td>\n",
       "      <td>ORGANIZATION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Entities        Labels\n",
       "0            China           GPE\n",
       "1  Dow Jones Index        PERSON\n",
       "2          America           GPE\n",
       "3            Apple        PERSON\n",
       "4    United States           GPE\n",
       "5             Zoom        PERSON\n",
       "6           Google  ORGANIZATION"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display entities in tabular format\n",
    "entities =[]\n",
    "labels =[]\n",
    "for chunk in chunks:\n",
    "    if hasattr(chunk,'label'):\n",
    "        #print(chunk)\n",
    "        entities.append(' '.join(c[0] for c in chunk))\n",
    "        labels.append(chunk.label())\n",
    "        \n",
    "entities_labels = list(set(zip(entities, labels)))\n",
    "entities_df = pd.DataFrame(entities_labels)\n",
    "entities_df.columns = [\"Entities\",\"Labels\"]\n",
    "entities_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
